{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_3_with_attention_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/Assignment-3/DL_Assignment_3_with_attention_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nES7nFYc9JeF"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras.layers import Concatenate,TimeDistributed\n",
        "import torch\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, GRU,SimpleRNN\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, RNN,GRU\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense,BatchNormalization\n",
        "from keras.layers import Flatten\n",
        "import math\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, GRU,SimpleRNN\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "import tensorflow.keras as K\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIMxBIKt9bub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab8463d-51c2-40d6-c26e-38f9d55636dd"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhqO5yOG9ejo"
      },
      "source": [
        "train=\"/content/gdrive/MyDrive/hi.translit.sampled.train.tsv\"\n",
        "test=\"/content/gdrive/MyDrive/hi.translit.sampled.test.tsv\"\n",
        "val=\"/content/gdrive/MyDrive/hi.translit.sampled.dev.tsv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT44gKDN9nf5"
      },
      "source": [
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "with open(train, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "\n",
        "    ##Appending '\\t' and '\\n' for every word in target_texts\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bjB_2ZS9w4_"
      },
      "source": [
        "\"\"\"Preparing set for unique imput and target characters\"\"\"\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "for i in range( len(input_texts)):\n",
        "  for char in input_texts[i]:\n",
        "    input_characters.add(char)\n",
        "\n",
        "  for char in target_texts[i]:\n",
        "    target_characters.add(char)\n",
        "\n",
        "\n",
        "#Convert set to list\n",
        "input_characters = list(input_characters)\n",
        "target_characters = list(target_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3WfYvpu9yNr"
      },
      "source": [
        "\"\"\"Preparing dictionary for input characters and target characters\"\"\"\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "##Finding the length of maximum input word and maximum target word\n",
        "\n",
        "max_encoder_seq_length=-1\n",
        "for i in input_texts:\n",
        "  if len(i)>max_encoder_seq_length:\n",
        "    max_encoder_seq_length=len(i)\n",
        "\n",
        "max_decoder_seq_length=-1\n",
        "for i in target_texts:\n",
        "  if len(i)>max_decoder_seq_length:\n",
        "    max_decoder_seq_length=len(i)\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "target_token_index[\" \"]=65\n",
        "input_token_index[\" \"]=26"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZTrQ_QY9__0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b24abb-9087-4e27-fadc-b78403798502"
      },
      "source": [
        "max_encoder_seq_length, max_decoder_seq_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uiop3z02_bAf"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts),max_decoder_seq_length,num_decoder_tokens), dtype=\"float32\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl22XPrsAP2a"
      },
      "source": [
        "# For validation data\n",
        "input_texts_val = []\n",
        "target_texts_val = []\n",
        "\n",
        "with open(val, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_text=input_text\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts_val.append(input_text)\n",
        "    target_texts_val.append(target_text)\n",
        "\n",
        "\n",
        "encoder_input_data_val = np.zeros(\n",
        "    (len(input_texts_val), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "\n",
        "for i, input_text in enumerate(input_texts_val):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_val[i, t] =  input_token_index[char]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPx-x1PZADvj"
      },
      "source": [
        "##For test data\n",
        "# For validation data\n",
        "input_texts_test = []\n",
        "target_texts_test = []\n",
        "\n",
        "with open(test, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_text=input_text\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts_test.append(input_text)\n",
        "    target_texts_test.append(target_text)\n",
        "\n",
        "\n",
        "encoder_input_data_test = np.zeros(\n",
        "    (len(input_texts_test), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "\n",
        "for i, input_text in enumerate(input_texts_test):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_test[i, t] =  input_token_index[char]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBxJoZF-Cqe"
      },
      "source": [
        "##For test data\n",
        "input_texts_test = []\n",
        "target_texts_test = []\n",
        "\n",
        "with open(test, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_text=input_text\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts_test.append(input_text)\n",
        "    target_texts_test.append(target_text)\n",
        "\n",
        "\n",
        "for i, input_text in enumerate(input_texts_test):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_test[i, t] =  input_token_index[char]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsemQlPR-CyY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e7JX0r--C1C"
      },
      "source": [
        "reverse_input_char_index=dict()\n",
        "for char, i in input_token_index.items():\n",
        "  reverse_input_char_index[i]=char\n",
        "\n",
        "reverse_target_char_index=dict()\n",
        "for char, i in target_token_index.items():\n",
        "  reverse_target_char_index[i]=char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VRX318C-C4H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikWCDtV-C6k"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, RNN\n",
        "from keras.layers import dot\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Attention,TimeDistributed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5_ThgTd0ZHQ9",
        "outputId": "674ff6ec-45c8-4dea-9d1d-043c3c219ec1"
      },
      "source": [
        "\"\"\"Before executing the next cell download attention.py from here: https://github.com/thushv89/attention_keras/blob/322a16ee147122026b63305aaa5e899d9e5de883/src/layers/attention.py\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Before executing the next cell download attention.py from here: https://github.com/thushv89/attention_keras/blob/322a16ee147122026b63305aaa5e899d9e5de883/src/layers/attention.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmmvqlIUZVaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA8LfruA-C-P",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "2ec9e59b-416e-4caa-a6c9-20e98b9aca4e"
      },
      "source": [
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('attention.py','wb').write(src)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-239f8334-bb3e-409b-b8a5-15a65f91af90\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-239f8334-bb3e-409b-b8a5-15a65f91af90\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving attention.py to attention.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upsmpRKE-DCN"
      },
      "source": [
        "from attention import AttentionLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vos_Lu_A4Jj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9SJoM8A4Mb"
      },
      "source": [
        "\n",
        "#### Testing of accuracy word by word check\n",
        "\n",
        "\n",
        "def ValAcc2(x, y, enc_model,dec_model, beam_size):\n",
        "  #onehotlist=[]\n",
        "  correct_predict=0\n",
        "  pred_words=[]\n",
        "  temp=\"\"\n",
        "  for i, input_text in enumerate(x):\n",
        "\n",
        "    input_seq = x[i:i+1]\n",
        "    decoded_word = decode_sequence1(input_seq, enc_model, dec_model,beam_size)\n",
        "\n",
        "    temp=decoded_word\n",
        "    #print(decoded_word)\n",
        "    #print(\"---\",y[i])\n",
        "    pred_words.append(decoded_word+'\\n')\n",
        "\n",
        "    true_word=y[i].split('\\t')[1].split('\\n')[0]\n",
        "    if decoded_word==true_word:\n",
        "      correct_predict=correct_predict+1\n",
        "\n",
        "    #print(\"Decoded_word: \", decoded_word, \"\\tTrue_word: \",true_word)\n",
        "  \n",
        "  \n",
        "  valid_acc=correct_predict/len(x)    \n",
        "  return valid_acc, pred_words  #, decoded_word #,correct_predict,temp\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6leL50lxA4O-"
      },
      "source": [
        "def beam_search_decoder(predictions, target_sequence_index,top_k=3):\n",
        "\n",
        "  #initially\n",
        "  output_sequences=[(\"\",0)]\n",
        "\n",
        "  for token_probs in predictions:\n",
        "    new_sequences=[]\n",
        "\n",
        "   \n",
        "    for old_seq, old_score in output_sequences:\n",
        "      for char_index in range(len(token_probs)):\n",
        "        new_seq = old_seq+target_sequence_index[char_index]\n",
        "\n",
        "        new_score= old_score + math.log(token_probs[char_index])\n",
        "        #logging based of ASCII Value might be incorrect --> Clear\n",
        "\n",
        "        new_sequences.append((new_seq, new_score))\n",
        "\n",
        "    \n",
        "    output_sequences=sorted(new_sequences, key= lambda val: val[1], reverse=True)\n",
        "\n",
        "    #Picking first k based on scores\n",
        "    output_sequences = output_sequences[:top_k]\n",
        "\n",
        "  \n",
        "  return output_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY7IJl9sCljt"
      },
      "source": [
        "def decode_sequence1(input_seq, enc_model, dec_model,beam_size):\n",
        "    attention_weights=[]\n",
        "    enc_out,states_value = enc_model.predict(input_seq)\n",
        "    out_tokens2d=[]\n",
        "    \n",
        "\n",
        "    #>>we will give 1 char at a time as we have defined in the inference model<<#\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0][0] = target_token_index['\\t']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    x=0\n",
        "   \n",
        "    while not stop_condition:\n",
        "        output_tokens, attention,h= dec_model.predict(\n",
        "          [enc_out,states_value,target_seq])\n",
        "        #print(\"predict pass\")\n",
        "        #print(output_tokens.shape)\n",
        "        # Sample a token\n",
        "        out_tokens2d.append(output_tokens)\n",
        "        sampled_token_index=np.argmax(output_tokens, axis=-1)[0, 0]\n",
        "        #print(sampled_token_index)\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        #print(sampled_char)\n",
        "        decoded_sentence = decoded_sentence +sampled_char\n",
        "\n",
        "       \n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length ):\n",
        "            stop_condition = True\n",
        "            # print(len(decoded_sentence))\n",
        "            #print(sampled_char)\n",
        "            # if (sampled_char=='\\n'):\n",
        "            #   print(\"n\")\n",
        "            # else :\n",
        "            #   print(\"exceeded\")\n",
        "\n",
        "        #print(sampled_char)\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        # target_seq[0,sampled_token_index] =1\n",
        "        #target_seq[0,0] =sampled_token_index\n",
        "        #target_seq = np.zeros((1,max_decoder_seq_length))\n",
        "        #pos=pos+1\n",
        "        target_seq[0][0] = target_token_index[sampled_char]\n",
        "        \n",
        "        \n",
        "        # Update states\n",
        "        states_value = [h]\n",
        "        attention_weights.append((sampled_token_index, attention))\n",
        "        x=x+1\n",
        "        \n",
        "    out_tokens2d=np.array(out_tokens2d)\n",
        "    #print(out_tokens2d.shape[0])\n",
        "    len_word=out_tokens2d.shape[0]\n",
        "    len_allchars=out_tokens2d.shape[3]\n",
        "    #print(input_seq.shape)\n",
        "    out_tokens2d=out_tokens2d.reshape(len_word,len_allchars)\n",
        "    #print(out_tokens2d.shape)\n",
        "    #return decoded_sentence\n",
        "    \"\"\"calling beam search\"\"\"\n",
        "    bsd_out=beam_search_decoder(out_tokens2d,reverse_target_char_index ,beam_size)\n",
        "    #decoded_sentence=bsd_out[0]\n",
        "    #return decoded_sentence\n",
        "    ans=bsd_out[0][0].split('\\n')\n",
        "    \n",
        "    # predWord=ans[0]\n",
        "    return ans[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WEuyC_7tDVbq",
        "outputId": "2454878a-2433-4dcd-aa5d-5ee6e9bb6905"
      },
      "source": [
        "\"\"\"General function for model creation and returning accuracy on test data\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'General function for model creation and returning accuracy on test data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpZwIu-dDVYn"
      },
      "source": [
        "def model_run(layer_size, embedding_size,epochs=5, beam_size=3):\n",
        "\n",
        "  ##Model creation\n",
        "  latent_dim=layer_size\n",
        "  encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
        "  # x = keras.layers.Embedding(num_encoder_tokens, 32,input_length = max_encoder_seq_length)(encoder_inputs)\n",
        "  x = keras.layers.Embedding(num_encoder_tokens, embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
        "\n",
        "  encoder = GRU(latent_dim, return_sequences=True,return_state=True)\n",
        "\n",
        "  encoder_states = []\n",
        "  ops = encoder(x)\n",
        "  encoder_outputs, state_h = ops[0],ops[1:]\n",
        "  encoder_states+=[state_h]\n",
        "\n",
        "  decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
        "  # y = keras.layers.Embedding(num_decoder_tokens, 32,input_length = max_decoder_seq_length)(decoder_inputs)\n",
        "  y = keras.layers.Embedding(num_decoder_tokens, embedding_size,input_length = max_decoder_seq_length)(decoder_inputs)\n",
        "\n",
        "  decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _ = decoder_gru(y,\n",
        "                                    initial_state=encoder_states[0])\n",
        "\n",
        "\n",
        "\n",
        "  attn_layer = AttentionLayer(name='attention_layer')\n",
        "  attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "  decoder_concat_input = keras.layers.Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "\n",
        "  decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "  decoder_outputs1 = decoder_dense(decoder_concat_input)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs1)\n",
        "  # model.summary()\n",
        "  model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "  )\n",
        "  model.fit(\n",
        "      [encoder_input_data, decoder_input_data],\n",
        "      decoder_target_data,\n",
        "      batch_size=32,\n",
        "      epochs=epochs,\n",
        "      validation_split=0.2,\n",
        "  )\n",
        "\n",
        "  #Inference model\n",
        "  #encoder model\n",
        "\n",
        "  encoder_inputs_inf = Input(batch_shape=(1,max_encoder_seq_length))\n",
        "\n",
        "  #>>Take the embedding layer from the trained model it has learned the weights during training.<<#\n",
        "  x = model.layers[2](encoder_inputs_inf)\n",
        "  enc_inf_states = []\n",
        "\n",
        "  ops = encoder(x)\n",
        "\n",
        "  encoder_outputs_inf = ops[0] \n",
        "  state_h_inf = ops[1:]\n",
        "  enc_inf_states+=[state_h_inf]\n",
        "  encoder_model = Model(inputs=encoder_inputs_inf, outputs=[encoder_outputs_inf,enc_inf_states])\n",
        "\n",
        "\n",
        "  #decoder model\n",
        "\n",
        "  encoder_states_inf = Input(batch_shape=(1,max_encoder_seq_length,latent_dim))\n",
        "\n",
        "  #>>for inf we will pass 1 character at a time<<#\n",
        "  decoder_inputs_inf=Input(batch_shape=(1,1))\n",
        "  #>>take 2nd embedding layer from the trained model<<#\n",
        "  y = model.layers[3](decoder_inputs_inf)\n",
        "\n",
        "\n",
        "  dec_states = []\n",
        "  dec_inf_states = []\n",
        "\n",
        "  decoder_state_input_h = [Input(batch_shape=(1,latent_dim))]\n",
        "\n",
        "  ops= decoder_gru(y, initial_state=decoder_state_input_h)\n",
        "\n",
        "  decoder_outputs = ops[0]\n",
        "  state_h = ops[1:]\n",
        "  dec_inf_states+=[state_h]\n",
        "  dec_states+=[decoder_state_input_h]\n",
        "\n",
        "  #this is also layer from previous trained model\n",
        "  attn_out, attn_states = attn_layer([encoder_states_inf, decoder_outputs])\n",
        "\n",
        "  #attn_out, attn_states = attn_layer([encoder_states_inf, decoder_outputs],return_attention_scores=True)\n",
        "  decoder_concat_input = keras.layers.Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "  #>>decoder_dense also from trained model<<#\n",
        "  decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "  decoder_model = Model(inputs=\n",
        "      [encoder_states_inf,dec_states,decoder_inputs_inf],\n",
        "      outputs=[decoder_outputs,attn_states,dec_inf_states])\n",
        "  \n",
        "  return ValAcc2(encoder_input_data_test, target_texts_test, encoder_model, decoder_model,beam_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ur7IgHRiDVVq",
        "outputId": "31e91a81-cf9a-43bd-b260-e087e915f643"
      },
      "source": [
        "\"\"\"For wandb\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'For wandb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hIdbShsDVRx",
        "outputId": "d9ddb6d7-fa5d-4942-abac-d4ddd475ccab"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.7MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (8.0.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.4MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 27.5MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=ea88b1b29251fb772cdefcfbe97adb5dc2effcec0fd6b59bdf0c6c4c69ff7787\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=b6d674dd1432cd8440987dbd193617bd549f088256fb5f76a46b90c00e6d23d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: docker-pycreds, configparser, shortuuid, subprocess32, sentry-sdk, smmap, gitdb, GitPython, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bydANTwDVO0",
        "outputId": "ae108cd2-ee8e-41e5-a312-43328f3827e8"
      },
      "source": [
        "import wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlnTutmUEHBp"
      },
      "source": [
        "sweep_config_temp={\n",
        "  \"name\": \"DL Assign-3-attention_20eps\",\n",
        "  \"method\": \"random\",\n",
        "  'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "  \"parameters\": {\n",
        "        \"input_embedding_size\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \"hidden_layer_size\":{\n",
        "            \"values\":[128,  256]  \n",
        "        },\n",
        "        \"beam_size\":{\n",
        "            \"values\":[3, 5, 10]\n",
        "        },\n",
        "        \n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJNsgbVJEKsb"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config_temp, entity=\"cs20m014\", project=\"DL Assign-3-attention_20eps\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAom_9PPEMqO"
      },
      "source": [
        "hyperparameter_defaults = dict(\n",
        "    input_embedding_size=64,\n",
        "    hidden_layer_size=128,\n",
        "    beam_size= 5,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjyUmzzxEQk7"
      },
      "source": [
        "def train():\n",
        "  wandb.init(config=hyperparameter_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "  embed_size=config.input_embedding_size\n",
        "  l_size=config.hidden_layer_size\n",
        "  beam_size=config.beam_size\n",
        "  epochs=20\n",
        "\n",
        "  # accuracy = model_run(5, n_filters, multiplier,drop,fSize, activation, batchNorm)\n",
        "  # val_accuracy = model_run(n_stack,layer_type,l_size,dropout,beam_size,epochs)\n",
        "  val_accuracy = model_run(l_size, embed_size,epochs, beam_size)\n",
        "\n",
        "  wandb.log({\n",
        "        \"Val Accuracy\": val_accuracy})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4EDfeXXEXt3"
      },
      "source": [
        "\"\"\"Only for resuming sweeps\"\"\"\n",
        "\n",
        "wandb.agent(\"3mvk7cgs\", project=\"DL Assign-3-attention_20eps\", function=train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YTkQsBaXFOXj",
        "outputId": "304ef78b-868e-4f46-f214-5039ea25809d"
      },
      "source": [
        "\"\"\"For calculating test accuracy on best model configuration\"\"\"\n",
        "\n",
        "accuracy, predictedWords = model_run(256, 64, 20, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'For calculating test accuracy on best model configuration'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VoeqmCCFPz9"
      },
      "source": [
        "\"\"\"5 (d) answer : attention heatmap for 10 random images from test data\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kezWK-EFOaZ"
      },
      "source": [
        "#Writing pred_words to .txt file\n",
        "with open('/content/result.txt', 'w') as writefile:\n",
        "  for word in predictedWords:\n",
        "    writefile.write(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Str0IFNAC6D5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHUPTGsaC6cR"
      },
      "source": [
        "def plot_attention_weights(n,encoder_inputs, attention_weights, en_id2word, fr_id2word, filename=None):\n",
        "   \n",
        "\n",
        "    if len(attention_weights) == 0:\n",
        "        print('Your attention weights was empty. No attention map saved to the disk. ' +\n",
        "              '\\nPlease check if the decoder produced  a proper translation')\n",
        "        return\n",
        "\n",
        "    mats = []\n",
        "    dec_inputs = []\n",
        "    \n",
        "    print(np.array(attention_weights).shape)\n",
        "    for dec_ind, attn in attention_weights:\n",
        "        x=attn.reshape(-1)\n",
        "        dec_inputs.append(dec_ind)\n",
        "        #x=attn[dec_ind][1][0][0]\n",
        "        temp=[]\n",
        "        for i in range(2,20):\n",
        "          temp.append(x[i])\n",
        "        mats.append(temp)\n",
        "\n",
        "          \n",
        "    #mats=temp\n",
        "    print(np.array(mats).shape)\n",
        "    attention_mat = np.transpose(np.array(mats))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(11, 11))\n",
        "    ax.imshow(attention_mat)\n",
        "    nirm = '/content/Nirmala.ttf'\n",
        "    hindi_font = FontProperties(fname=nirm)\n",
        "\n",
        "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
        "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
        "    l=len(input_texts[n])\n",
        "    ax.set_xticklabels([fr_id2word[inp] if inp != 0 else \"<Res>\"  for inp in dec_inputs], fontproperties=hindi_font)\n",
        "\n",
        "    #ax.set_yticklabels([en_id2word[inp] if inp != 0 else \"<Res>\" if i>=l  else 0 for i,inp in enumerate(encoder_inputs).ravel()])\n",
        "    i=0\n",
        "    english=[]\n",
        "    for inp in encoder_inputs.ravel():\n",
        "      if i>=l:\n",
        "        temp=\"<Res>\"\n",
        "      elif inp!=0:\n",
        "         temp=en_id2word[inp]\n",
        "      else:\n",
        "        temp=\"a\"\n",
        "      i=i+1\n",
        "      english.append(temp)\n",
        "    ax.set_yticklabels(english)\n",
        "\n",
        "    # c=0\n",
        "    # for i in range(20):\n",
        "    #   if i > len(dec_inputs):\n",
        "    #     temp=\"<Res>\"\n",
        "    #     else:\n",
        "    #       temp=fr_id2word[i]\n",
        "\n",
        "    # ax.set_xticklabels([fr_id2word[inp] c=c+1 if c>len(dec_inputs) elsefor inp in dec_inputs], fontproperties=hindi_font)\n",
        "    # ax.set_yticklabels([en_id2word[inp] for inp in encoder_inputs.ravel()])\n",
        "\n",
        "    ax.tick_params(labelsize=11)\n",
        "    ax.tick_params(axis='x', labelrotation=90)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi3Mg0iWEALd"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy_cqlVcC6fY"
      },
      "source": [
        "for i in range(1,10):\n",
        "   n = random.randint(0,4500)\n",
        "   decoded_sentence ,output_values,result_list,attn_wts= decode_sequence(n:n+1)\n",
        "   plot_attention_weights(n,encoder_input_data[n:n+1], attn_wts, reverse_input_char_index, reverse_target_char_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCYBH_6pE-tt"
      },
      "source": [
        "def upload_files():\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    open(k, 'wb').write(v)\n",
        "  return list(uploaded.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajaZcwyLC6iu"
      },
      "source": [
        "img1 = mpimg.imread('/content/1.png')\n",
        "img2 = mpimg.imread('/content/2.png')\n",
        "img3 = mpimg.imread('/content/3.png')\n",
        "img4 = mpimg.imread('/content/4.png')\n",
        "img5 = mpimg.imread('/content/5.png')\n",
        "img6 = mpimg.imread('/content/6.png')\n",
        "img7 = mpimg.imread('/content/7.png')\n",
        "img8 = mpimg.imread('/content/8.png')\n",
        "img9 = mpimg.imread('/content/9.png')\n",
        "img10 = mpimg.imread('/content/10.png')\n",
        "\n",
        "# imgplot = plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cly2NoDzC6l9"
      },
      "source": [
        "img=[]\n",
        "img.append(img1)\n",
        "img.append(img2)\n",
        "img.append(img3)\n",
        "img.append(img4)\n",
        "img.append(img5)\n",
        "img.append(img6)\n",
        "img.append(img7)\n",
        "img.append(img8)\n",
        "img.append(img9)\n",
        "img.append(img10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCZaDUYLC6o6"
      },
      "source": [
        "\n",
        "i=0\n",
        "\n",
        "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 60))\n",
        "for idx,ax in enumerate(axes.ravel()): \n",
        "    imgl = img[i]\n",
        "    ax.imshow(imgl, cmap = plt.get_cmap('gray'))\n",
        "    #ax.title.set_text('Pred='+ xpredict[i]+'  ,  Act='+yplot[i])\n",
        "    i=i+1\n",
        "    if i >9 :\n",
        "      break\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncA2l-DVbJmw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6MLV8ymbJjx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vH9G5OuXbJg6",
        "outputId": "0fc22414-ca7e-40f9-c0b6-898de2e381ae"
      },
      "source": [
        "\"For Question 6 - Connectivity\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'For Question 6 - Connectivity'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pufpbhhvf4yo"
      },
      "source": [
        "def attention_decode_sequence(input_seq):\n",
        "    attention_weights=[]\n",
        "    result_list, output_values = [], []\n",
        "    enc_out,states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0][0] = target_token_index['\\t']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, attention,h= decoder_model.predict(\n",
        "          [enc_out,states_value,target_seq])\n",
        "        #output = attention[0][0][0]\n",
        "        #print(output)\n",
        "        #print(np.array(attention).shape)\n",
        "\t\t    # output = sigmoid(output)\n",
        "\t\t    # output_values.append(output)\n",
        "      \n",
        "\n",
        "        sampled_token_index=np.argmax(output_tokens, axis=-1)[0, 0]\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        result_list.append(sampled_char)\n",
        "        decoded_sentence = decoded_sentence +sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length ):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0][0] = target_token_index[sampled_char]\n",
        "        states_value = [h]\n",
        "        attention_weights.append((sampled_token_index, attention))\n",
        "        #attention_weights.append((attention))\n",
        "\n",
        "    return decoded_sentence,output_values,result_list,attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WFmujpzbTQP"
      },
      "source": [
        "\"\"\"Function of print color for a given parameter\"\"\"\n",
        "def prtColor(param):\n",
        "\tdisplay(html_print(''.join([cstr(txt, color=clr) for txt,clr in param])))\n",
        " \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy5qY3rwc-Cx"
      },
      "source": [
        "\"\"\"Function that visualise for a current character\"\"\"\n",
        "def currentStr(s, color='black'):\n",
        "  if s == ' ':\n",
        "    return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "  else:\n",
        "    return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7P6APKFdAEP"
      },
      "source": [
        "def getColor(x):\n",
        "\tcolors = ['#85c5e1', '#88c7e2', '#91cbe5', '#97cde6', '#a9d3e8'\n",
        "    '#b7d6ec', '#bbdcee', '#c7e9f1', '#eff3fc', '#f9d8d8',\n",
        "    '#f7e8e6', '#f9e7d8', '#f9cdb9', '#f9a8a9', '#f99f9f',\n",
        "    '#f87696', '#fa5f9f', '#fb4943', '#fc3b9b', '#fe2e9e']\n",
        "\t\n",
        "\treturn colors[x]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAHPVm6eZ-J"
      },
      "source": [
        "def visualize(attentionWts, pred):\n",
        "\ttxt_clrs = []\n",
        "\tfor i in range(len(attentionWts)):\n",
        "\t\ttext = (pred[i], getColor(attentionWts[i]))\n",
        "\t\ttxt_clrs.append(text)\n",
        "\tprint_color(txt_clrs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPCLQLetfnKr"
      },
      "source": [
        "#First calling attention decode sequence to get the attention weights and the prediction word\n",
        "inp=1111 #This can be set by the user on their own choice\n",
        "decoded_word, output_values,result_list,attention_weights = attention_decode_sequence(encoder_input_data[inp:inp+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Wvo3BsfSo7"
      },
      "source": [
        "visualize(attention_weights, decoder_word)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}