{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_3_with_attention_final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/Assignment-3/DL_Assignment_3_with_attention_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nES7nFYc9JeF"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras.layers import Concatenate,TimeDistributed\n",
        "import torch\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, GRU,SimpleRNN\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, RNN,GRU\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense,BatchNormalization\n",
        "from keras.layers import Flatten\n",
        "import math\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, GRU,SimpleRNN\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "import tensorflow.keras as K\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIMxBIKt9bub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab8463d-51c2-40d6-c26e-38f9d55636dd"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhqO5yOG9ejo"
      },
      "source": [
        "train=\"/content/gdrive/MyDrive/hi.translit.sampled.train.tsv\"\n",
        "test=\"/content/gdrive/MyDrive/hi.translit.sampled.test.tsv\"\n",
        "val=\"/content/gdrive/MyDrive/hi.translit.sampled.dev.tsv\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT44gKDN9nf5"
      },
      "source": [
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "with open(train, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "\n",
        "    ##Appending '\\t' and '\\n' for every word in target_texts\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bjB_2ZS9w4_"
      },
      "source": [
        "\"\"\"Preparing set for unique imput and target characters\"\"\"\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "for i in range( len(input_texts)):\n",
        "  for char in input_texts[i]:\n",
        "    input_characters.add(char)\n",
        "\n",
        "  for char in target_texts[i]:\n",
        "    target_characters.add(char)\n",
        "\n",
        "\n",
        "#Convert set to list\n",
        "input_characters = list(input_characters)\n",
        "target_characters = list(target_characters)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3WfYvpu9yNr"
      },
      "source": [
        "\"\"\"Preparing dictionary for input characters and target characters\"\"\"\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "##Finding the length of maximum input word and maximum target word\n",
        "\n",
        "max_encoder_seq_length=-1\n",
        "for i in input_texts:\n",
        "  if len(i)>max_encoder_seq_length:\n",
        "    max_encoder_seq_length=len(i)\n",
        "\n",
        "max_decoder_seq_length=-1\n",
        "for i in target_texts:\n",
        "  if len(i)>max_decoder_seq_length:\n",
        "    max_decoder_seq_length=len(i)\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "target_token_index[\" \"]=65\n",
        "input_token_index[\" \"]=26"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZTrQ_QY9__0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b24abb-9087-4e27-fadc-b78403798502"
      },
      "source": [
        "max_encoder_seq_length, max_decoder_seq_length"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uiop3z02_bAf"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts),max_decoder_seq_length,num_decoder_tokens), dtype=\"float32\"\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl22XPrsAP2a"
      },
      "source": [
        "# For validation data\n",
        "input_texts_val = []\n",
        "target_texts_val = []\n",
        "\n",
        "with open(val, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_text=input_text\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts_val.append(input_text)\n",
        "    target_texts_val.append(target_text)\n",
        "\n",
        "\n",
        "encoder_input_data_val = np.zeros(\n",
        "    (len(input_texts_val), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "\n",
        "for i, input_text in enumerate(input_texts_val):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_val[i, t] =  input_token_index[char]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPx-x1PZADvj"
      },
      "source": [
        "##For test data\n",
        "# For validation data\n",
        "input_texts_test = []\n",
        "target_texts_test = []\n",
        "\n",
        "with open(test, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_text=input_text\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts_test.append(input_text)\n",
        "    target_texts_test.append(target_text)\n",
        "\n",
        "\n",
        "encoder_input_data_test = np.zeros(\n",
        "    (len(input_texts_test), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "\n",
        "for i, input_text in enumerate(input_texts_test):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_test[i, t] =  input_token_index[char]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBxJoZF-Cqe"
      },
      "source": [
        "##For test data\n",
        "input_texts_test = []\n",
        "target_texts_test = []\n",
        "\n",
        "with open(test, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    input_text=input_text\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts_test.append(input_text)\n",
        "    target_texts_test.append(target_text)\n",
        "\n",
        "\n",
        "for i, input_text in enumerate(input_texts_test):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_test[i, t] =  input_token_index[char]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsemQlPR-CyY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e7JX0r--C1C"
      },
      "source": [
        "reverse_input_char_index=dict()\n",
        "for char, i in input_token_index.items():\n",
        "  reverse_input_char_index[i]=char\n",
        "\n",
        "reverse_target_char_index=dict()\n",
        "for char, i in target_token_index.items():\n",
        "  reverse_target_char_index[i]=char"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VRX318C-C4H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikWCDtV-C6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA8LfruA-C-P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upsmpRKE-DCN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}