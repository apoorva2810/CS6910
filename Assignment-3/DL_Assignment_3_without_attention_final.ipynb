{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_3_without_attention_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/Assignment-3/DL_Assignment_3_without_attention_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj4HyabDW-dF"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, GRU,SimpleRNN\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, RNN,GRU\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense,BatchNormalization\n",
        "from keras.layers import Flatten\n",
        "import math\n",
        "\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "import tensorflow.keras as K\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OnccMDDXUDs",
        "outputId": "143636a9-8d38-4084-b13f-228a50028eb0"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhIzJTU8XZsG"
      },
      "source": [
        "data_path1=\"/content/gdrive/MyDrive/hi.translit.sampled.dev.tsv\"\n",
        "data_path2=\"/content/gdrive/MyDrive/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "with open(data_path2, \"r\", encoding=\"utf-8\") as f:\n",
        "    test = f.read().split(\"\\n\")\n",
        "\n",
        "data_path3=\"/content/gdrive/MyDrive/hi.translit.sampled.train.tsv\"\n",
        "\n",
        "with open(data_path3, \"r\", encoding=\"utf-8\") as f:\n",
        "    train = f.read().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-j_Rw1MYPTh",
        "outputId": "9d46223d-b9c3-467a-dd6f-7142f6ddbd33"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwqZdDSwXpdP"
      },
      "source": [
        "# batch_size = 64  # Batch size for training.\n",
        "# latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXz5ZgogYMT3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcAJ_6ymXsVh"
      },
      "source": [
        "data_path=data_path3\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text,_ = line.split(\"\\t\")\n",
        "\n",
        "    ##Appending '\\t' and '\\n' for every word in target_texts\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QhPsvba2mp3o",
        "outputId": "c5ede8be-ba81-443b-d2e9-ee0143401ff3"
      },
      "source": [
        "input_texts[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'om'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoRMH7-hbTz5"
      },
      "source": [
        "\"\"\"Preparing set for unique imput and target characters\"\"\"\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "for i in range( len(input_texts)):\n",
        "  for char in input_texts[i]:\n",
        "    input_characters.add(char)\n",
        "\n",
        "  for char in target_texts[i]:\n",
        "    target_characters.add(char)\n",
        "\n",
        "\n",
        "#Convert set to list\n",
        "input_characters = list(input_characters)\n",
        "target_characters = list(target_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19MhzAZscVpv"
      },
      "source": [
        "\"\"\"Preparing dictionary for input characters and target characters\"\"\"\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "##Finding the length of maximum input word and maximum target word\n",
        "\n",
        "max_encoder_seq_length=-1\n",
        "for i in input_texts:\n",
        "  if len(i)>max_encoder_seq_length:\n",
        "    max_encoder_seq_length=len(i)\n",
        "\n",
        "max_decoder_seq_length=-1\n",
        "for i in target_texts:\n",
        "  if len(i)>max_decoder_seq_length:\n",
        "    max_decoder_seq_length=len(i)\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "target_token_index[\" \"]=65\n",
        "input_token_index[\" \"]=26"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP_o31BAaOiT",
        "outputId": "0cd9b02d-e835-47e4-b852-955b1da84d4b"
      },
      "source": [
        "max_encoder_seq_length, max_decoder_seq_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YG8iGELaOlr"
      },
      "source": [
        "#Train and validation split of training data\n",
        "inputTrain, inputValTrain, targetTrain, targetValTest = train_test_split(input_texts, target_texts,\n",
        "                                                    test_size = 0.1,\n",
        "                                                    random_state = 1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUzuVuO0ihPH",
        "outputId": "9ffc23ec-816e-4d6b-8f2b-bccd63a129cc"
      },
      "source": [
        "print(len(inputTrain), len(targetTrain))\n",
        "print(len(inputValTrain), len(targetValTest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39783 39783\n",
            "4421 4421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRAlX2rXihSA"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(inputTrain), max_encoder_seq_length, num_encoder_tokens+1), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(inputTrain), max_decoder_seq_length, num_decoder_tokens+1), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(inputTrain), max_decoder_seq_length, num_decoder_tokens+1), dtype=\"float32\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8BQOfZMi5lk",
        "outputId": "893c8929-6407-4e2a-c792-994e688344af"
      },
      "source": [
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)\n",
        "print(decoder_target_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39783, 20, 27)\n",
            "(39783, 21, 66)\n",
            "(39783, 21, 66)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvaHG3LpihUx"
      },
      "source": [
        "\"\"\"Creating one hot vector for encoder inputs, decoder inputs, and decoder targer\"\"\"\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(inputTrain, targetTrain)):\n",
        "\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "   \n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(inputTrain, targetTrain)):\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    \n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCzl-ixuihoY"
      },
      "source": [
        "#For inputValTrain\n",
        "\n",
        "encoder_input_val = np.zeros(\n",
        "    (len(inputValTrain), max_encoder_seq_length, num_encoder_tokens+1), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, input_text in enumerate(inputValTrain):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_val[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_val[i, t + 1 :, input_token_index[\" \"]] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_OfskHuihrG"
      },
      "source": [
        "\"\"\"Function returns word level accuracy and predicted words\"\"\"\n",
        "\n",
        "def  ValAcc2(x,y,num_stack,model_type,enc_model,dec_model,beam_size):\n",
        "  \n",
        "  correct_predict=0\n",
        "  predicted_words=[]\n",
        "  for i, input_text in enumerate(x):\n",
        "    \n",
        "    # for t, char in enumerate(input_text):\n",
        "    #     onehotlist[i, t, input_token_index[char]] = 1.0\n",
        "    # onehotlist[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "\n",
        "    input_seq = x[i:i+1]\n",
        "    if num_stack==2:\n",
        "      decoded_word = decode_sequence2(input_seq,model_type,enc_model,dec_model,beam_size)\n",
        "    else :\n",
        "       decoded_word = decode_sequence1(input_seq,model_type,enc_model,dec_model,beam_size)\n",
        "\n",
        "    \n",
        "    #print(decoded_word)\n",
        "    #print(\"---\",y[i])\n",
        "    predicted_words.append(decoded_word)\n",
        "\n",
        "    if decoded_word==y[i].split('\\t')[1].split('\\n')[0]:\n",
        "      correct_predict=correct_predict+1\n",
        "  \n",
        "  \n",
        "  valid_acc=correct_predict/len(x)    \n",
        "  return valid_acc, predicted_words\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jYehdA1ihuX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}