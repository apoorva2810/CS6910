{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "partA_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/Assignment-2/partA/partA_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANyXSLdF1qZq"
      },
      "source": [
        "#Before executing the code, allocate GPU in Colab from: Edit -> Notebook Settings -> Hardware accelerator -> GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsOSjZml2E9F"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense,BatchNormalization\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "import tensorflow.keras as K\n",
        "import cv2\n",
        "import tensorflow.keras.backend as Kb\n",
        "from keras import layers\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn import preprocessing  \n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03_4BtSq3UuB"
      },
      "source": [
        "#To access content from google drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mreYDBi73qlM"
      },
      "source": [
        "#Todo- first rename the file in drive\n",
        "#Use this link: https://drive.google.com/drive/folders/1yu5DuYlM_cvV5qwIKKnl3sHRG_G41aLB?usp=sharing\n",
        "#To make copy of pickled data and save it folder name 'Raw_data350'\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/xtrain','rb') as xT:\n",
        "  xTrain = pickle.load(xT)\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/ytrain','rb') as yT:\n",
        "  yTrain = pickle.load(yT)\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/xtest','rb') as xV:\n",
        "  xTest = pickle.load(xV)\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/ytest','rb') as yV:\n",
        "  yTest = pickle.load(yV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W42SpIgx36u0"
      },
      "source": [
        "xplot=[]\n",
        "yplot=[]\n",
        "x=10\n",
        "for i in range(30):\n",
        "  xplot.append(xTest[x])\n",
        "  yplot.append(yTest[x])\n",
        "  x=x+25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoQYmcFa5Vyj"
      },
      "source": [
        "len(yTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckp2Nuk75FdH"
      },
      "source": [
        "xTrain, xVal, yTrain, yVal= train_test_split(xTrain, yTrain, test_size=.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNE4az7w5S-N"
      },
      "source": [
        "len(xTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo28MjX15FZa"
      },
      "source": [
        "#Removing all the images which have only 2 dimensions and their respective y label from Train data\n",
        "\n",
        "for i in range(len(xTrain)):\n",
        "\n",
        "  xTrain[i]=np.array(xTrain[i])\n",
        "  \n",
        "  if(xTrain[i].ndim==2):\n",
        "    del xTrain[i]\n",
        "    del yTrain[i]\n",
        "\n",
        "xTrain=np.array(xTrain)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqICrURG5FDY"
      },
      "source": [
        "#Removing all the images which have only 2 dimensions and their respective y label from Validation data\n",
        "\n",
        "for i in range(len(xVal)):\n",
        "\n",
        "  xVal[i]=np.array(xVal[i])\n",
        "  \n",
        "  if(xVal[i].ndim==2):\n",
        "    del xVal[i]\n",
        "    del yVal[i]\n",
        "\n",
        "xVal=np.array(xVal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1iq7qQZ5FAL"
      },
      "source": [
        "#Removing all the images which have only 2 dimensions and their respective y label from Train data\n",
        "\n",
        "for i in range(len(xTest)):\n",
        "\n",
        "  xTest[i]=np.array(xTest[i])\n",
        "  \n",
        "  if(xTest[i].ndim==2):\n",
        "    del xTest[i]\n",
        "    del yTest[i]\n",
        "\n",
        "xTest=np.array(xTest)\n",
        "type(xTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1OloQFs5E7J"
      },
      "source": [
        "# Normalising xTrain\n",
        "for i in range(len(xTrain)):\n",
        "  # xTrain[i]= xTrain[i].astype('float32')\n",
        "  xTrain[i]= xTrain[i]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFgjHtMY5azT"
      },
      "source": [
        "# Normalising xVal\n",
        "for i in range(len(xVal)):\n",
        "\n",
        "  xVal[i]=xVal[i].astype('float32')\n",
        "  xVal[i]=xVal[i]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJCIp8Ks5c_q"
      },
      "source": [
        "# Normalising xTest\n",
        "for i in range(len(xTest)):\n",
        "\n",
        "  xTest[i]=xTest[i].astype('float32')\n",
        "  xTest[i]=xTest[i]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrwxSgZ5av0"
      },
      "source": [
        "# Assigning the string labels of yTest and yTrain to integers\n",
        "pple = preprocessing.LabelEncoder()\n",
        "yTest=pple.fit_transform(yTest)\n",
        "yVal=pple.fit_transform(yVal)\n",
        "yTrain=pple.fit_transform(yTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhBU7mx-5atD"
      },
      "source": [
        "#Change the integer y to one-hot vector\n",
        "def to_categorical(y, n_class):\n",
        "    return np.eye(n_class, dtype='uint8')[y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPbVnLRN6Gyc"
      },
      "source": [
        "# getting the categorical value of yTrain and yTest using to_categorical() function\n",
        "yTrain_cat=[]\n",
        "for i in range(len(yTrain)):\n",
        "  cat_d=to_categorical(int(yTrain[i]),10)\n",
        "  yTrain_cat.append(cat_d)\n",
        "\n",
        "yVal_cat=[]\n",
        "for i in range(len(yVal)):\n",
        "  cat_d=to_categorical(int(yVal[i]),10)\n",
        "  yVal_cat.append(cat_d)\n",
        "\n",
        "yTest_cat=[]\n",
        "for i in range(len(yTest)):\n",
        "  cat_d=to_categorical(int(yTest[i]),10)\n",
        "  yTest_cat.append(cat_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZTS3ujS6Gtu"
      },
      "source": [
        "#changing the yTrain_cat and yTest_cat to numpy array \n",
        "\n",
        "yTrain_cat=np.array(yTrain_cat)\n",
        "yVal_cat=np.array(yVal_cat)\n",
        "yTest_cat=np.array(yTest_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pjP7MPX6Gp9"
      },
      "source": [
        "print(yTrain_cat.shape)\n",
        "print(yVal_cat.shape)\n",
        "print(yTest_cat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiEdSDY6GnM"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aSYQHTT6GjV"
      },
      "source": [
        "import wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXS-S1fu6ntd"
      },
      "source": [
        "sweep_config_temp={\n",
        "  \"name\": \"DL Assign-2-partA-final1\",\n",
        "  \"method\": \"random\",\n",
        "  'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "  \"parameters\": {\n",
        "        \"num_filters\": {\n",
        "            \"values\": [64,128,256]\n",
        "        },\n",
        "        \"filter_organisation\":{\n",
        "            \"values\":['same','half']\n",
        "        },\n",
        "        \"activationFunction\":{\n",
        "            \"values\":['leakyRelu','Relu']  \n",
        "        },\n",
        "        \"data_augumentation\":{\n",
        "            \"values\":['No']  \n",
        "        },\n",
        "        \"filterSize\":{\n",
        "            \"values\": [(2,2),(3,3),(4,4)]  \n",
        "        },\n",
        "        \"dropout\":{\n",
        "            \"values\":[0.2,0.3,0.5]\n",
        "        },\n",
        "        \"batch_normalization\":{\n",
        "            \"values\":['Yes', 'No']\n",
        "        },\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKBhLQtw6np6"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config_temp, entity=\"cs20m014\", project=\"DL Assign-2-partA-final\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dm5PnKD6nnN"
      },
      "source": [
        "hyperparameter_defaults = dict(\n",
        "    num_filters=64,\n",
        "    filter_organisation='same',\n",
        "    activationFunction='leakyRelu',\n",
        "    data_augumentation='No',\n",
        "    filterSize=(3,3),\n",
        "    dropout=0.5,\n",
        "    batch_normalization= 'Yes',\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TseX4fpy67G4"
      },
      "source": [
        "def model_run(eps, filter_size, mul_filter, drop, fSize, actv, batchNorm):\n",
        "\n",
        "  current_filter_size=filter_size\n",
        "\n",
        "  activationFun='relu'\n",
        "\n",
        "  if actv=='leakyRelu':\n",
        "    activationFun=LeakyReLU(alpha=0.1)\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization(input_shape=(350, 350, 3)))\n",
        "\n",
        "  \n",
        "\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun, input_shape=(350, 350, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  \n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        " \n",
        "  model.add(MaxPooling2D((3, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "\n",
        "  model.add(Dense(1024, activation=activationFun))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(xTrain,yTrain_cat,epochs = eps , validation_data = (xVal, yVal_cat))\n",
        "\n",
        "  wandb.log({\n",
        "        \n",
        "        \"Train Loss\": history.history['loss'][eps-1],\n",
        "        \"Train Acc\": history.history['accuracy'][eps-1],\n",
        "        \"Valid Loss\":history.history['val_loss'][eps-1],\n",
        "        \"Valid Acc\": history.history['val_accuracy'][eps-1]})\n",
        "  return history.history['val_accuracy'][eps-1]\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpyt4ENH67Ah"
      },
      "source": [
        "def train():\n",
        "  wandb.init(config=hyperparameter_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "  n_filters=config.num_filters\n",
        "  multiplier=0\n",
        "\n",
        "  if config.filter_organisation=='same':\n",
        "    multiplier=1\n",
        "  elif config.filter_organisation=='half':\n",
        "    multiplier=0.5\n",
        "  elif config.filter_organisation=='double':\n",
        "    multiplier=2\n",
        "  \n",
        "  drop=config.dropout\n",
        "  fSize=config.filterSize\n",
        "  activation=config.activationFunction\n",
        "  batchNorm = config.batch_normalization\n",
        "  accuracy = model_run(5, n_filters, multiplier,drop,fSize, activation, batchNorm)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uKGg4UA667d"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl2Dq0A47JJg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knnwaATjawy9"
      },
      "source": [
        "**\"\"\"BEST MODEL\"\"\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK2XF-NQbJSD"
      },
      "source": [
        "part A 4a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7kdpgYXa26b"
      },
      "source": [
        "  eps=20\n",
        "  filter_size= 128\n",
        "  mul_filter=0.5 \n",
        "  drop=.2\n",
        "  fSize=(2,2)\n",
        "  actv= 'leakyRelu'\n",
        "  batchNorm='Yes'\n",
        "\n",
        "  current_filter_size=filter_size\n",
        "\n",
        "  activationFun='relu'\n",
        "\n",
        "  if actv=='leakyRelu':\n",
        "    activationFun=LeakyReLU(alpha=0.1)\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization(input_shape=(350, 350, 3)))\n",
        "\n",
        "  \n",
        "\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun, input_shape=(350, 350, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  \n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        " \n",
        "  model.add(MaxPooling2D((3, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "\n",
        "  model.add(Dense(1024, activation=activationFun))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(xTrain,yTrain_cat,epochs = eps , validation_data = (xTest, yTest_cat))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfEWNPppa3fW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrkUMDad72Me"
      },
      "source": [
        "PART A 4C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJcWtkrb7JGU"
      },
      "source": [
        "model1 = Model(inputs=model.inputs, outputs=model.layers[1].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpE5_Nv8lCI"
      },
      "source": [
        "oneimg=xTest[60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtTF2OXR8lFl"
      },
      "source": [
        "oneimg = np.expand_dims(oneimg, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty-e-_BV8lIY"
      },
      "source": [
        "filter_vis = model1.predict(oneimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-bkuZg8lL6"
      },
      "source": [
        "\n",
        "filter_vis.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpEqJDOU8lO9"
      },
      "source": [
        "#128 filters in best model , hence plotting as 16 x 8 grid\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=16, ncols=8, figsize=(12, 20))\n",
        "for idx,i in enumerate(axes.ravel()): \n",
        "    #print(i) visualising index by index\n",
        "    imgl =filter_vis[0, :, :, idx-1]\n",
        "    i.imshow(imgl, cmap = plt.get_cmap('gray'))\n",
        " \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62JQmTM28lRx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD6D47zD-JYb"
      },
      "source": [
        "4B **PARTA** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh0qMH8W9-SV"
      },
      "source": [
        "xpredict=[]\n",
        "k=10\n",
        "for i in range(30):\n",
        "  test_image = xTest[k]\n",
        "  test_image = np.expand_dims(test_image, axis=0)\n",
        "  prediction = model.predict(test_image)\n",
        "  prediction = np.argmax(prediction[0])\n",
        "  xpredict.append(prediction)\n",
        "  k=k+25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCud1pc9-Xv"
      },
      "source": [
        "for i in range(len(xpredict)):\n",
        "  if xpredict[i]==0:\n",
        "    xpredict[i]='Amphibia'\n",
        "  if xpredict[i]==1:\n",
        "    xpredict[i]='Animalia'\n",
        "  if xpredict[i]==2:\n",
        "      xpredict[i]='Arachnida'\n",
        "  if xpredict[i]==3:\n",
        "      xpredict[i]='Aves'\n",
        "  if xpredict[i]==4:\n",
        "      xpredict[i]='Fungi'\n",
        "  if xpredict[i]==5:\n",
        "      xpredict[i]='Insecta'\n",
        "  if xpredict[i]==6:\n",
        "      xpredict[i]='Mammalia'\n",
        "  if xpredict[i]==7 :\n",
        "      xpredict[i]='Mollusca'\n",
        "  if xpredict[i]==8 :\n",
        "      xpredict[i]='Plantae'\n",
        "  if xpredict[i]==9:\n",
        "      xpredict[i]='Reptilia'\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joBSJgZt9-a4"
      },
      "source": [
        "\n",
        "i=0\n",
        "\n",
        "fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(12, 60))\n",
        "for idx,ax in enumerate(axes.ravel()): \n",
        "    \n",
        "    imgl = xplot[i]\n",
        "    ax.imshow(imgl, cmap = plt.get_cmap('gray'))\n",
        "    ax.title.set_text('Pred='+ xpredict[i]+'  ,  Act='+yplot[i])\n",
        "    i=i+1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlUUf49Z9-dX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZQRtFL_gKx"
      },
      "source": [
        "**best model **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tBdi2pr9-gM"
      },
      "source": [
        "  eps=20\n",
        "  filter_size= 128\n",
        "  mul_filter=0.5 \n",
        "  drop=.2\n",
        "  fSize=(2,2)\n",
        "  actv= 'leakyRelu'\n",
        "  batchNorm='Yes'\n",
        "\n",
        "  current_filter_size=filter_size\n",
        "\n",
        "  activationFun='relu'\n",
        "\n",
        "  if actv=='leakyRelu':\n",
        "    activationFun=LeakyReLU(alpha=0.1)\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization(input_shape=(350, 350, 3)))\n",
        "\n",
        "  \n",
        "\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun, input_shape=(350, 350, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  \n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        " \n",
        "  model.add(MaxPooling2D((3, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "\n",
        "  model.add(Dense(1024, activation=activationFun))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(xTrain,yTrain_cat,epochs = eps , validation_data = (xTest, yTest_cat))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09yzJ5Bhtl7D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJhI-NO9tl3F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkJ3ay4Jtmnx"
      },
      "source": [
        "## Question 5-Guided Backprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEKsDRdnvyab"
      },
      "source": [
        "#Before running this code run the best model code in above cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGMwY5E0tlxV"
      },
      "source": [
        "img=xplot[9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OBPvK_9ujJ3"
      },
      "source": [
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axIP0i8wujOp"
      },
      "source": [
        "img = np.array(img)\n",
        "x = np.expand_dims(img, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrSKhKt1ujTv"
      },
      "source": [
        "reluActv=tf.keras.activations.relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWTKP-5rujYo"
      },
      "source": [
        "inputs = tf.cast(x, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzBi1E75ujdM"
      },
      "source": [
        "# inputs = np.float32(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxA3_xIWujiq"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqTtRy8uujnt"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXwv7WKujrk"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iUJLBvujvb"
      },
      "source": [
        "inp=model.inputs\n",
        "last_layer=model.get_layer('conv2d_19').output    #Choose the last convolution layer name from model.summary() for parameter in get_layer()\n",
        "\n",
        "backProp_model = Model(\n",
        "    inputs = [inp],    \n",
        "    outputs = [last_layer]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyBPZ_i4vGvm"
      },
      "source": [
        "outputs = backProp_model(inputs)[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlgbfTrEvG0U"
      },
      "source": [
        "actLayer=[]\n",
        "for l in backProp_model.layers[1:]:\n",
        "  if hasattr(l, 'activation'):\n",
        "    actLayer.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVcZhXNavG51"
      },
      "source": [
        "actLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqpn9mNyvG-9"
      },
      "source": [
        "#  outputs = backProp_model(inputs)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUdIrRIxvHCt"
      },
      "source": [
        "#Defining function for guided relu and it's gradient\n",
        "@tf.custom_gradient\n",
        "def GR(x):\n",
        "  def gradient(dy):\n",
        "    return np.float32(x)  * np.float32(dy) *np.float32(dy) \n",
        "  return tf.nn.relu(x), gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TbRkrq0vHHD"
      },
      "source": [
        "for l in actLayer:\n",
        "    l.activation = GR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOZw_N_6vHQz"
      },
      "source": [
        "backProp_model(inputs)[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMZcMKHTvHUi"
      },
      "source": [
        "#Here we are actually calculating the gradient for back-propagation\n",
        "with tf.GradientTape() as t:\n",
        "  t.watch(inputs)   \n",
        "  outputs = backProp_model(inputs)[0]\n",
        "grads = t.gradient(outputs,inputs)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnLWiW2XvXp_"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0CAbx09vXue"
      },
      "source": [
        "outputs.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICmvTYuLvXzm"
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEIqlsMBvX4L"
      },
      "source": [
        "grads.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O-QVnS8vX86"
      },
      "source": [
        "guided_back_prop[:,:,0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufm3YeSSvYUu"
      },
      "source": [
        "#Visualizing the guided back prop out\n",
        "guided_back_prop =grads\n",
        "\n",
        "R=guided_back_prop[:,:,0]\n",
        "G=guided_back_prop[:,:,1]\n",
        "B=guided_back_prop[:,:,2]\n",
        "\n",
        "temp = np.dstack((R, G, B))       \n",
        "temp = temp - np.min(temp)\n",
        "temp = temp / temp.max()\n",
        "    \n",
        "backProp_image = plt.imshow(temp)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4TeiIlvYYd"
      },
      "source": [
        "print(backProp_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgif0LlFvYb7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y3bFaBWvHZh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}