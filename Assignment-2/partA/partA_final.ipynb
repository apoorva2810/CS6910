{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "partA_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP54MMcwmYO9bURqlZUnlyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/Assignment-2/partA/partA_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANyXSLdF1qZq"
      },
      "source": [
        "#Before executing the code, allocate GPU in Colab from: Edit -> Notebook Settings -> Hardware accelerator -> GPU"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsOSjZml2E9F"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense,BatchNormalization\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "import tensorflow.keras as K\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn import preprocessing  \n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03_4BtSq3UuB"
      },
      "source": [
        "#To access content from google drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mreYDBi73qlM"
      },
      "source": [
        "#Todo- first rename the file in drive\n",
        "#Use this link: https://drive.google.com/drive/folders/1yu5DuYlM_cvV5qwIKKnl3sHRG_G41aLB?usp=sharing\n",
        "#To make copy of pickled data and save it folder name 'Raw_data350'\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/xtrain','rb') as xT:\n",
        "  xTrain = pickle.load(xT)\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/ytrain','rb') as yT:\n",
        "  yTrain = pickle.load(yT)\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/xtest','rb') as xV:\n",
        "  xTest = pickle.load(xV)\n",
        "\n",
        "with open('/content/gdrive/My Drive/Raw_data350/ytest','rb') as yV:\n",
        "  yTest = pickle.load(yV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W42SpIgx36u0"
      },
      "source": [
        "xplot=[]\n",
        "yplot=[]\n",
        "x=10\n",
        "for i in range(30):\n",
        "  xplot.append(xTest[x])\n",
        "  yplot.append(yTest[x])\n",
        "  x=x+25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoQYmcFa5Vyj"
      },
      "source": [
        "len(yTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckp2Nuk75FdH"
      },
      "source": [
        "xTrain, xVal, yTrain, yVal= train_test_split(xTrain, yTrain, test_size=.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNE4az7w5S-N"
      },
      "source": [
        "len(xTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo28MjX15FZa"
      },
      "source": [
        "#Removing all the images which have only 2 dimensions and their respective y label from Train data\n",
        "\n",
        "for i in range(len(xTrain)):\n",
        "\n",
        "  xTrain[i]=np.array(xTrain[i])\n",
        "  \n",
        "  if(xTrain[i].ndim==2):\n",
        "    del xTrain[i]\n",
        "    del yTrain[i]\n",
        "\n",
        "xTrain=np.array(xTrain)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqICrURG5FDY"
      },
      "source": [
        "#Removing all the images which have only 2 dimensions and their respective y label from Validation data\n",
        "\n",
        "for i in range(len(xVal)):\n",
        "\n",
        "  xVal[i]=np.array(xVal[i])\n",
        "  \n",
        "  if(xVal[i].ndim==2):\n",
        "    del xVal[i]\n",
        "    del yVal[i]\n",
        "\n",
        "xVal=np.array(xVal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1iq7qQZ5FAL"
      },
      "source": [
        "#Removing all the images which have only 2 dimensions and their respective y label from Train data\n",
        "\n",
        "for i in range(len(xTest)):\n",
        "\n",
        "  xTest[i]=np.array(xTest[i])\n",
        "  \n",
        "  if(xTest[i].ndim==2):\n",
        "    del xTest[i]\n",
        "    del yTest[i]\n",
        "\n",
        "xTest=np.array(xTest)\n",
        "type(xTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1OloQFs5E7J"
      },
      "source": [
        "# Normalising xTrain\n",
        "for i in range(len(xTrain)):\n",
        "  # xTrain[i]= xTrain[i].astype('float32')\n",
        "  xTrain[i]= xTrain[i]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFgjHtMY5azT"
      },
      "source": [
        "# Normalising xVal\n",
        "for i in range(len(xVal)):\n",
        "\n",
        "  xVal[i]=xVal[i].astype('float32')\n",
        "  xVal[i]=xVal[i]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJCIp8Ks5c_q"
      },
      "source": [
        "# Normalising xTest\n",
        "for i in range(len(xTest)):\n",
        "\n",
        "  xTest[i]=xTest[i].astype('float32')\n",
        "  xTest[i]=xTest[i]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrwxSgZ5av0"
      },
      "source": [
        "# Assigning the string labels of yTest and yTrain to integers\n",
        "pple = preprocessing.LabelEncoder()\n",
        "yTest=pple.fit_transform(yTest)\n",
        "yVal=pple.fit_transform(yVal)\n",
        "yTrain=pple.fit_transform(yTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhBU7mx-5atD"
      },
      "source": [
        "#Change the integer y to one-hot vector\n",
        "def to_categorical(y, n_class):\n",
        "    return np.eye(n_class, dtype='uint8')[y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPbVnLRN6Gyc"
      },
      "source": [
        "# getting the categorical value of yTrain and yTest using to_categorical() function\n",
        "yTrain_cat=[]\n",
        "for i in range(len(yTrain)):\n",
        "  cat_d=to_categorical(int(yTrain[i]),10)\n",
        "  yTrain_cat.append(cat_d)\n",
        "\n",
        "yVal_cat=[]\n",
        "for i in range(len(yVal)):\n",
        "  cat_d=to_categorical(int(yVal[i]),10)\n",
        "  yVal_cat.append(cat_d)\n",
        "\n",
        "yTest_cat=[]\n",
        "for i in range(len(yTest)):\n",
        "  cat_d=to_categorical(int(yTest[i]),10)\n",
        "  yTest_cat.append(cat_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZTS3ujS6Gtu"
      },
      "source": [
        "#changing the yTrain_cat and yTest_cat to numpy array \n",
        "\n",
        "yTrain_cat=np.array(yTrain_cat)\n",
        "yVal_cat=np.array(yVal_cat)\n",
        "yTest_cat=np.array(yTest_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pjP7MPX6Gp9"
      },
      "source": [
        "print(yTrain_cat.shape)\n",
        "print(yVal_cat.shape)\n",
        "print(yTest_cat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiEdSDY6GnM"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aSYQHTT6GjV"
      },
      "source": [
        "import wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXS-S1fu6ntd"
      },
      "source": [
        "sweep_config_temp={\n",
        "  \"name\": \"DL Assign-2-partA-final1\",\n",
        "  \"method\": \"random\",\n",
        "  'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "  \"parameters\": {\n",
        "        \"num_filters\": {\n",
        "            \"values\": [64,128,256]\n",
        "        },\n",
        "        \"filter_organisation\":{\n",
        "            \"values\":['same','half']\n",
        "        },\n",
        "        \"activationFunction\":{\n",
        "            \"values\":['leakyRelu','Relu']  \n",
        "        },\n",
        "        \"data_augumentation\":{\n",
        "            \"values\":['No']  \n",
        "        },\n",
        "        \"filterSize\":{\n",
        "            \"values\": [(2,2),(3,3),(4,4)]  \n",
        "        },\n",
        "        \"dropout\":{\n",
        "            \"values\":[0.2,0.3,0.5]\n",
        "        },\n",
        "        \"batch_normalization\":{\n",
        "            \"values\":['Yes', 'No']\n",
        "        },\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKBhLQtw6np6"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config_temp, entity=\"cs20m014\", project=\"DL Assign-2-partA-final\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dm5PnKD6nnN"
      },
      "source": [
        "hyperparameter_defaults = dict(\n",
        "    num_filters=64,\n",
        "    filter_organisation='same',\n",
        "    activationFunction='leakyRelu',\n",
        "    data_augumentation='No',\n",
        "    filterSize=(3,3),\n",
        "    dropout=0.5,\n",
        "    batch_normalization= 'Yes',\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TseX4fpy67G4"
      },
      "source": [
        "def model_run(eps, filter_size, mul_filter, drop, fSize, actv, batchNorm):\n",
        "\n",
        "  current_filter_size=filter_size\n",
        "\n",
        "  activationFun='relu'\n",
        "\n",
        "  if actv=='leakyRelu':\n",
        "    activationFun=LeakyReLU(alpha=0.1)\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization(input_shape=(350, 350, 3)))\n",
        "\n",
        "  \n",
        "\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun, input_shape=(350, 350, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        "  \n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(drop))\n",
        "  if batchNorm=='Yes':\n",
        "    model.add(BatchNormalization())\n",
        "  \n",
        "  current_filter_size=current_filter_size*mul_filter\n",
        "  model.add(Conv2D(current_filter_size, fSize, activation=activationFun))\n",
        " \n",
        "  model.add(MaxPooling2D((3, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(drop))\n",
        "\n",
        "\n",
        "  model.add(Dense(1024, activation=activationFun))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(xTrain,yTrain_cat,epochs = eps , validation_data = (xVal, yVal_cat))\n",
        "\n",
        "  wandb.log({\n",
        "        \n",
        "        \"Train Loss\": history.history['loss'][eps-1],\n",
        "        \"Train Acc\": history.history['accuracy'][eps-1],\n",
        "        \"Valid Loss\":history.history['val_loss'][eps-1],\n",
        "        \"Valid Acc\": history.history['val_accuracy'][eps-1]})\n",
        "  return history.history['val_accuracy'][eps-1]\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpyt4ENH67Ah"
      },
      "source": [
        "def train():\n",
        "  wandb.init(config=hyperparameter_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "  n_filters=config.num_filters\n",
        "  multiplier=0\n",
        "\n",
        "  if config.filter_organisation=='same':\n",
        "    multiplier=1\n",
        "  elif config.filter_organisation=='half':\n",
        "    multiplier=0.5\n",
        "  elif config.filter_organisation=='double':\n",
        "    multiplier=2\n",
        "  \n",
        "  drop=config.dropout\n",
        "  fSize=config.filterSize\n",
        "  activation=config.activationFunction\n",
        "  batchNorm = config.batch_normalization\n",
        "  accuracy = model_run(5, n_filters, multiplier,drop,fSize, activation, batchNorm)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uKGg4UA667d"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl2Dq0A47JJg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJcWtkrb7JGU"
      },
      "source": [
        "#Sukanya's part"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}