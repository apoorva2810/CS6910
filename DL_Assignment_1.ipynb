{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj6_a7YAhs0l",
        "outputId": "b8e7c5a8-35b0-4f28-aaa7-ba0d5f1fef8c"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.7MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.4MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 27.6MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=0f385ed1821774e1315771ef74c9ec32c57a6c88fbacad3c26e1f57ca43feacc\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=8471ae729b16c3647642a8b019d793a9bf56231abca33109a9f3b34650a361d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, smmap, gitdb, GitPython, docker-pycreds, shortuuid, sentry-sdk, pathtools, configparser, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqd_v_yiiJMD",
        "outputId": "6db610ed-735a-4ae7-d3d6-0179dc574e0e"
      },
      "source": [
        "import wandb\r\n",
        "!wandb login"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "Aborted!\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnpKwPOyiOiK"
      },
      "source": [
        "#importing the fasihion mnist dataset from keras\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxqKNYSNidYq"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.colors\r\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_0Br5Pnjdp3"
      },
      "source": [
        "def sigmoid(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        if(x[i]>=0):\r\n",
        "            temp[i]=1.0/(1.0+np.exp(-x[i]))\r\n",
        "        else:\r\n",
        "            temp[i]=np.exp(x[i])/(1.0+np.exp(x[i]))\r\n",
        "    return temp"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqUUIJOXkHc-"
      },
      "source": [
        "def sigmoid_dif(x):\r\n",
        "    return x*(1-x)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl1Gx7PVkJ3N"
      },
      "source": [
        "def tanh(x):\r\n",
        "    # temp=np.zeros((len(x),1))\r\n",
        "    # for i in range(len(x)):\r\n",
        "    #     temp[i]=np.exp(x[i])-np.exp(-x[i])/np.exp(x[i])+np.exp(-x[i])\r\n",
        "    # return temp\r\n",
        "    return np.tanh(x)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tEZpDMbkWR6"
      },
      "source": [
        "def tanh_dif(x):\r\n",
        "    return (1 - (x)**2)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGMLh35LkY7I"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def relu(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        if x[i]>0:\r\n",
        "            temp[i]=x[i]\r\n",
        "    return temp"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ON1PRhNkaY_"
      },
      "source": [
        "\r\n",
        "def relu_dif(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        if x[i]>0:\r\n",
        "            temp[i]=1\r\n",
        "    return temp\r\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4NVDZf0kcA4"
      },
      "source": [
        "def softmax(x):\r\n",
        "    mx=np.max(x)\r\n",
        "    x=x-mx\r\n",
        "    return np.exp(x)/np.sum(np.exp(x))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa2zQAxJk2YA"
      },
      "source": [
        "def initialize_network(layer_size_list,mthd):\r\n",
        "    weights={}\r\n",
        "    bias={}\r\n",
        "    if mthd=='random':\r\n",
        "        for i,x,y in zip(range(len(layer_size_list)-1),layer_size_list[:-1],layer_size_list[1:]):\r\n",
        "            weights[i+1]=np.random.randn(y,x)\r\n",
        "        for i,x in zip(range(len(layer_size_list)-1),layer_size_list[1:]):\r\n",
        "            bias[i+1]=np.zeros((x,1))\r\n",
        "        return weights,bias\r\n",
        "    elif mthd=='xavier':\r\n",
        "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
        "        for i,x,y in zip(range(len(layer_size_list)-1),layer_size_list[:-1],layer_size_list[1:]):\r\n",
        "            weights[i+1]=np.array(initializer(shape=(y, x)))\r\n",
        "        for i,x in zip(range(len(layer_size_list)-1),layer_size_list[1:]):\r\n",
        "            bias[i+1]=np.zeros((x,1))\r\n",
        "        return weights,bias"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83yCWbEblD9f"
      },
      "source": [
        "def forward_propagation(x,weights,bias,mthd):\r\n",
        "    a={}\r\n",
        "    h={}\r\n",
        "    h[0]=x\r\n",
        "    layers=len(weights)\r\n",
        "    if mthd=='sigmoid':\r\n",
        "        for i in range(layers-1):\r\n",
        "            a[i+1]=np.matmul(weights[i+1],h[i])+bias[i+1]\r\n",
        "            h[i+1]=sigmoid(a[i+1])\r\n",
        "        a[layers]=np.matmul(weights[layers],h[layers-1])+bias[layers]\r\n",
        "        h[layers]=softmax(a[layers])\r\n",
        "        #return h[layers]\r\n",
        "        return a,h\r\n",
        "    elif mthd=='tanh':\r\n",
        "        for i in range(layers-1):\r\n",
        "            a[i+1]=np.matmul(weights[i+1],h[i])+bias[i+1]\r\n",
        "            h[i+1]=tanh(a[i+1])\r\n",
        "        a[layers]=np.matmul(weights[layers],h[layers-1])+bias[layers]\r\n",
        "        h[layers]=softmax(a[layers])\r\n",
        "        #return h[layers]\r\n",
        "        return a,h\r\n",
        "    elif mthd=='relu':\r\n",
        "        for i in range(layers-1):\r\n",
        "            a[i+1]=np.matmul(weights[i+1],h[i])+bias[i+1]\r\n",
        "            h[i+1]=relu(a[i+1])\r\n",
        "        a[layers]=np.matmul(weights[layers],h[layers-1])+bias[layers]\r\n",
        "        h[layers]=softmax(a[layers])\r\n",
        "        #return h[layers]\r\n",
        "        return a,h\r\n",
        "        "
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RIlKNf7lfAi"
      },
      "source": [
        "def back_propagation(x,y,weights,bias,mthd):\r\n",
        "    a,h=forward_propagation(x.reshape(784,1),weights,bias,mthd)\r\n",
        "    dW={}\r\n",
        "    dB={}\r\n",
        "    dH={}\r\n",
        "    dA={}\r\n",
        "    layers=len(weights)\r\n",
        "    y_label=[0]*10\r\n",
        "    y_label[y]=1\r\n",
        "    y_label=np.array(y_label).reshape(10,1)\r\n",
        "    dA[layers]=h[layers].reshape(10,1)-y_label\r\n",
        "    for i in range(layers,0,-1):\r\n",
        "        dW[i]=np.matmul(dA[i],h[i-1].T)\r\n",
        "        dB[i]=dA[i]\r\n",
        "        dH[i-1]=np.matmul(weights[i].T,dA[i])\r\n",
        "        if mthd=='sigmoid':\r\n",
        "            dA[i-1]=np.multiply(dH[i-1],sigmoid_dif(h[i-1]))\r\n",
        "        elif mthd=='tanh':\r\n",
        "            dA[i-1]=np.multiply(dH[i-1],tanh_dif(h[i-1]))\r\n",
        "        elif mthd=='relu':\r\n",
        "            dA[i-1]=np.multiply(dH[i-1],relu_dif(h[i-1]))\r\n",
        "    return dW,dB    "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkBJfy-mEVN"
      },
      "source": [
        "#Optimization Functions"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a4QNcuMlrBW"
      },
      "source": [
        "def stochastic_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0,batch_size=10):\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        num_point_seen=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:30000],train_labels[0:30000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "            num_point_seen+=1\r\n",
        "            if num_point_seen==batch_size:\r\n",
        "                for z in range(len(layers_size_list)-1):\r\n",
        "                    weights[z+1]=weights[z+1]-learning_rate*dw[z+1]/batch_size-learning_rate*alpha*weights[z+1]\r\n",
        "                    bias[z+1]=bias[z+1]-learning_rate*db[z+1]/batch_size\r\n",
        "                for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "                    dw[i+1]=np.zeros((y,x))\r\n",
        "                for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "                    db[i+1]=np.zeros((x,1))\r\n",
        "                num_point_seen=0\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:30000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:30000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        '''for i in range(len(train_labels[0:30000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)'''\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        '''for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)'''\r\n",
        "        ce_train=log_loss(train_labels[0:30000],np.array(res).reshape((30000,10)))\r\n",
        "        ce_val=log_loss(test_labels,np.array(res_test).reshape((10000,10)))\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:30000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:30000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train,\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val,\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzSS97SkmMGu"
      },
      "source": [
        "def momentum_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        prev_vw={}\r\n",
        "        prev_vb={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "            prev_vw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "            prev_vb[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:30000],train_labels[0:30000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        m=len(train_images[0:30000])\r\n",
        "        v_w={}\r\n",
        "        v_b={}\r\n",
        "        gamma=0.9\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            v_w[i+1]=gamma*prev_vw[i+1]+learning_rate*dw[i+1]#/m\r\n",
        "            v_b[i+1]=gamma*prev_vb[i+1]+learning_rate*db[i+1]#/m\r\n",
        "            weights[i+1]=weights[i+1]-v_w[i+1]-learning_rate*alpha*weights[i+1] #this is l2 regularisation\r\n",
        "            bias[i+1]=bias[i+1]-v_b[i+1]\r\n",
        "            prev_vw[i+1]=v_w[i+1]\r\n",
        "            prev_vb[i+1]=v_b[i+1]\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:30000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:30000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        '''for i in range(len(train_labels[0:30000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)'''\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        '''for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)'''\r\n",
        "        ce_train=log_loss(train_labels[0:30000],np.array(res).reshape((30000,10)))\r\n",
        "        ce_val=log_loss(test_labels,np.array(res_test).reshape((10000,10)))\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:30000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:30000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train,\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val,\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWYuhwRxmk6t"
      },
      "source": [
        "def nestrov_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        prev_vw={}\r\n",
        "        prev_vb={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "            prev_vw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "            prev_vb[i+1]=np.zeros((x,1))\r\n",
        "        v_w={}\r\n",
        "        v_b={}\r\n",
        "        gamma=0.9\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            v_w[i+1]=gamma*prev_vw[i+1]\r\n",
        "            v_b[i+1]=gamma*prev_vb[i+1]\r\n",
        "        tempw={}\r\n",
        "        tempb={}\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            tempw[i+1]=weights[i+1]-v_w[i+1]\r\n",
        "            tempb[i+1]=bias[i+1]-v_b[i+1]\r\n",
        "        for j,k in zip(train_images[0:30000],train_labels[0:30000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,tempw,tempb,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        m=len(train_images[0:30000])\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            v_w[i+1]=gamma*prev_vw[i+1]+learning_rate*dw[i+1]#/m\r\n",
        "            v_b[i+1]=gamma*prev_vb[i+1]+learning_rate*db[i+1]#/m\r\n",
        "            weights[i+1]=weights[i+1]-v_w[i+1]-learning_rate*alpha*weights[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-v_b[i+1]\r\n",
        "            prev_vw[i+1]=v_w[i+1]\r\n",
        "            prev_vb[i+1]=v_b[i+1]\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:30000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:30000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        '''for i in range(len(train_labels[0:30000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)'''\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        '''for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)'''\r\n",
        "        ce_train=log_loss(train_labels[0:30000],np.array(res).reshape((30000,10)))\r\n",
        "        ce_val=log_loss(test_labels,np.array(res_test).reshape((10000,10)))\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:30000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:30000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train,\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val,\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZtKK_ism7YD"
      },
      "source": [
        "def rmsprop(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    res=[]\r\n",
        "    vw={}\r\n",
        "    vb={}\r\n",
        "    for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "        vw[i+1]=np.zeros((y,x))\r\n",
        "    for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "        vb[i+1]=np.zeros((x,1))\r\n",
        "    eps=1e-8\r\n",
        "    beta1=0.9\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:30000],train_labels[0:30000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        #m=len(train_images[0:30000])\r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw[l+1]=vw[l+1]*beta1+(1-beta1)*(dw[l+1]**2)\r\n",
        "            vb[l+1]=vb[l+1]*beta1+(1-beta1)*(db[l+1]**2)\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            weights[i+1]=weights[i+1]-(learning_rate)*(dw[i+1]/np.sqrt(vw[i+1]+eps))-learning_rate*alpha*weights[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-(learning_rate)*(db[i+1]/np.sqrt(vb[i+1]+eps))\r\n",
        "        \r\n",
        "        #below code is for testing accuracy on train data\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:30000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:30000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        '''for i in range(len(train_labels[0:30000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)'''\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        '''for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)'''\r\n",
        "        ce_train=log_loss(train_labels[0:30000],np.array(res).reshape((30000,10)))\r\n",
        "        ce_val=log_loss(test_labels,np.array(res_test).reshape((10000,10)))\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:30000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:30000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train,\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val,\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57p1Jb0am-j_"
      },
      "source": [
        "def adam(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    res=[]\r\n",
        "    vw={}\r\n",
        "    vb={}\r\n",
        "    mw={}\r\n",
        "    mb={}\r\n",
        "    mw_hat={}\r\n",
        "    mb_hat={}\r\n",
        "    vw_hat={}\r\n",
        "    vb_hat={}\r\n",
        "    for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "        vw[i+1]=np.zeros((y,x))\r\n",
        "        mw[i+1]=np.zeros((y,x))\r\n",
        "        mw_hat[i+1]=np.zeros((y,x))\r\n",
        "        vw_hat[i+1]=np.zeros((y,x))\r\n",
        "        \r\n",
        "    for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "        vb[i+1]=np.zeros((x,1))\r\n",
        "        mb[i+1]=np.zeros((x,1))\r\n",
        "        mb_hat[i+1]=np.zeros((x,1))\r\n",
        "        vb_hat[i+1]=np.zeros((x,1))\r\n",
        "\r\n",
        "    eps=1e-8\r\n",
        "    beta1=0.9\r\n",
        "    beta2=0.999\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:30000],train_labels[0:30000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        #m=len(train_images[0:30000])\r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw[l+1]=mw[l+1]*beta1+(1-beta1)*(dw[l+1])\r\n",
        "            mb[l+1]=mb[l+1]*beta1+(1-beta1)*(db[l+1])\r\n",
        "            \r\n",
        "        \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw[l+1]=vw[l+1]*beta2+(1-beta2)*(dw[l+1]**2)\r\n",
        "            vb[l+1]=vb[l+1]*beta2+(1-beta2)*(db[l+1]**2)\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw_hat[l+1]=mw[l+1]/(1-(beta1**(e+1)))\r\n",
        "            mb_hat[l+1]=mb[l+1]/(1-(beta1**(e+1)))\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw_hat[l+1]=vw[l+1]/(1-(beta2**(e+1)))\r\n",
        "            vb_hat[l+1]=vb[l+1]/(1-(beta2**(e+1)))\r\n",
        "            \r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            weights[i+1]=weights[i+1]-(learning_rate)*(mw_hat[i+1]/np.sqrt(vw_hat[i+1]+eps))-learning_rate*alpha*weights[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-(learning_rate)*(mb_hat[i+1]/np.sqrt(vb_hat[i+1]+eps))\r\n",
        "        \r\n",
        "        #below code is for testing accuracy on train data\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:30000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:30000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        '''for i in range(len(train_labels[0:30000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)'''\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        '''for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)'''\r\n",
        "        ce_train=log_loss(train_labels[0:30000],np.array(res).reshape((30000,10)))\r\n",
        "        ce_val=log_loss(test_labels,np.array(res_test).reshape((10000,10)))\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:30000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:30000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train,\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val,\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFDKpJ0cnBtY"
      },
      "source": [
        "def nadam(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    res=[]\r\n",
        "    vw={}\r\n",
        "    vb={}\r\n",
        "    mw={}\r\n",
        "    mb={}\r\n",
        "    mw_hat={}\r\n",
        "    mb_hat={}\r\n",
        "    vw_hat={}\r\n",
        "    vb_hat={}\r\n",
        "    for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "        vw[i+1]=np.zeros((y,x))\r\n",
        "        mw[i+1]=np.zeros((y,x))\r\n",
        "        mw_hat[i+1]=np.zeros((y,x))\r\n",
        "        vw_hat[i+1]=np.zeros((y,x))\r\n",
        "        \r\n",
        "    for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "        vb[i+1]=np.zeros((x,1))\r\n",
        "        mb[i+1]=np.zeros((x,1))\r\n",
        "        mb_hat[i+1]=np.zeros((x,1))\r\n",
        "        vb_hat[i+1]=np.zeros((x,1))\r\n",
        "\r\n",
        "    eps=1e-8\r\n",
        "    beta1=0.9\r\n",
        "    beta2=0.999\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:30000],train_labels[0:30000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        #m=len(train_images[0:30000])\r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw[l+1]=mw[l+1]*beta1+(1-beta1)*(dw[l+1])\r\n",
        "            mb[l+1]=mb[l+1]*beta1+(1-beta1)*(db[l+1])\r\n",
        "            \r\n",
        "        \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw[l+1]=vw[l+1]*beta2+(1-beta2)*(dw[l+1]**2)\r\n",
        "            vb[l+1]=vb[l+1]*beta2+(1-beta2)*(db[l+1]**2)\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw_hat[l+1]=mw[l+1]/(1-(beta1**(e+1)))\r\n",
        "            mb_hat[l+1]=mb[l+1]/(1-(beta1**(e+1)))\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw_hat[l+1]=vw[l+1]/(1-(beta2**(e+1)))\r\n",
        "            vb_hat[l+1]=vb[l+1]/(1-(beta2**(e+1)))\r\n",
        "        #xk = xk - (n/(vc_k**0.5 + epsilon))*(b1*mc_k + (1-b1)*gt/(1-b1**iter_count))  \r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            numrw=beta1*mw_hat[i+1]+(1-beta1)/(1-beta1**(e+1))*dw[i+1]\r\n",
        "            weights[i+1]=weights[i+1]-(learning_rate)*(numrw/np.sqrt(vw_hat[i+1]+eps))-learning_rate*alpha*weights[i+1]\r\n",
        "            \r\n",
        "            numrb=beta1*mb_hat[i+1]+(1-beta1)/(1-beta1**(e+1))*db[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-(learning_rate)*(numrb/np.sqrt(vb_hat[i+1]+eps))\r\n",
        "        \r\n",
        "        #below code is for testing accuracy on train data\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:30000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:30000])\r\n",
        "        print(e+1,\":\",train_acc)\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        '''for i in range(len(train_labels[0:30000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)'''\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        '''for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)'''\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        ce_train=log_loss(train_labels[0:30000],np.array(res).reshape((30000,10)))\r\n",
        "        ce_val=log_loss(test_labels,np.array(res_test).reshape((10000,10)))\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:30000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:30000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train,\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val,\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-vhrVgCntHH"
      },
      "source": [
        "def mse(pred,label):\r\n",
        "    label=np.array(label).reshape(10,1)\r\n",
        "    mse=0\r\n",
        "    for i in range(len(label)):\r\n",
        "        mse+=(pred[i]-label[i])**2\r\n",
        "    return mse[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvfpvsQJoPeH"
      },
      "source": [
        "def cross_entropy(pred,label):\r\n",
        "    yl=np.multiply(pred,np.array(label).reshape(10,1))\r\n",
        "    yl=yl[yl!=0]\r\n",
        "    yl=-np.log(yl)\r\n",
        "    #yl=np.mean(yl)\r\n",
        "    return yl[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-SGBv5LoWQl"
      },
      "source": [
        "def test_accuracy(weights,bias,test_images,test_labels,mthd):\r\n",
        "    res=[]\r\n",
        "    for i in test_images:\r\n",
        "        a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "        res.append(h[len(weights)])\r\n",
        "    y_pred=[]\r\n",
        "    for i in res:\r\n",
        "        j=list(i)\r\n",
        "        temp=j.index(max(j))\r\n",
        "        y_pred.append(temp)\r\n",
        "    test_acc=accuracy_score(y_pred, test_labels)\r\n",
        "    return test_acc"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmilxPNF5FCc"
      },
      "source": [
        "sweep_config_temp={\r\n",
        "  \"name\": \"DL Assign-1-ed1\",\r\n",
        "  \"method\": \"random\",\r\n",
        "  'metric': {\r\n",
        "      'name': 'accuracy',\r\n",
        "      'goal': 'maximize'   \r\n",
        "    },\r\n",
        "  \"parameters\": {\r\n",
        "        \"epochs\": {\r\n",
        "            \"values\": [5,10]\r\n",
        "        },\r\n",
        "        \"learning_rate\":{\r\n",
        "            \"values\":[0.00075,0.001]\r\n",
        "        },\r\n",
        "        \"layers_size_list\":{\r\n",
        "            \"values\":[[784,128,64,10],[784,32,32,32,10]]\r\n",
        "        },\r\n",
        "        \"optimizer\":{\r\n",
        "            \"values\":['rmsprop','adam','nadam']\r\n",
        "        },\r\n",
        "        \"batch_size\":{\r\n",
        "            \"values\":[16]\r\n",
        "            \r\n",
        "        },\r\n",
        "        \"weight_decay\":{\r\n",
        "            \"values\":[0,0.0005]\r\n",
        "        },\r\n",
        "        \"activation_function\":{\r\n",
        "            \"values\":['tanh','relu','sigmoid']\r\n",
        "        },\r\n",
        "        \"weight_initialisation\":{\r\n",
        "            \"values\":['random','xavier']\r\n",
        "        },\r\n",
        "    }\r\n",
        "}"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKOVSWHW5FOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1626b515-14f2-4a37-f51e-add28609c8a8"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config_temp, entity=\"cs20m014\", project=\"DL Assign-1-ed1\")"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: ad3rqm83\n",
            "Sweep URL: https://wandb.ai/cs20m014/DL%20Assign-1-ed1/sweeps/ad3rqm83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hBtfcqO5FU8"
      },
      "source": [
        "hyperparameter_defaults = dict(\r\n",
        "    epochs=5,\r\n",
        "    layers_size_list=[784,128,64,10],\r\n",
        "    learning_rate = 0.0001,\r\n",
        "    optimizer='adam',\r\n",
        "    batch_size=1,\r\n",
        "    weight_decay=0.001,\r\n",
        "    activation_function='tanh',\r\n",
        "    weight_initialisation='random',\r\n",
        "    )"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKVM6AVa5wH9"
      },
      "source": [
        "def train():\r\n",
        "  wandb.init(config=hyperparameter_defaults)\r\n",
        "  config = wandb.config\r\n",
        "  weights,bias=initialize_network(config.layers_size_list,config.weight_initialisation)\r\n",
        "  if config.optimizer=='sgd':\r\n",
        "    weights,bias=stochastic_gradient_descent(weights,bias,config.epochs,config.layers_size_list,train_images,train_labels,config.learning_rate,config.activation_function,config.weight_decay)\r\n",
        "    # stochastic_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0,batch_size=10\r\n",
        "  elif config.optimizer=='mgd':\r\n",
        "    weights,bias=momentum_gradient_descent(weights,bias,config.epochs,config.layers_size_list,train_images,train_labels,config.learning_rate,config.activation_function,config.weight_decay)\r\n",
        "    # momentum_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0)\r\n",
        "  elif config.optimizer=='nag':\r\n",
        "    weights,bias=nestrov_gradient_descent(weights,bias,config.epochs,config.layers_size_list,train_images,train_labels,config.learning_rate,config.activation_function,config.weight_decay)\r\n",
        "    # nestrov_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0)\r\n",
        "  elif config.optimizer=='rmsprop':\r\n",
        "    weights,bias=rmsprop(weights,bias,config.epochs,config.layers_size_list,train_images,train_labels,config.learning_rate,config.activation_function,config.weight_decay)\r\n",
        "\r\n",
        "  elif config.optimizer=='adam':\r\n",
        "    weights,bias=adam(weights,bias,config.epochs,config.layers_size_list,train_images,train_labels,config.learning_rate,config.activation_function,config.weight_decay)\r\n",
        "  elif config.optimizer=='nadam':\r\n",
        "    weights,bias=nadam(weights,bias,config.epochs,config.layers_size_list,train_images,train_labels,config.learning_rate,config.activation_function,config.weight_decay)\r\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB9dw4ng5wLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "2930d9c6-2f57-4cb0-9c7a-cf9101b74a9b"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1eaac8263fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quusu9U85FXR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}