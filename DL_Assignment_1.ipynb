{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva2810/CS6910/blob/main/DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj6_a7YAhs0l",
        "outputId": "232f2c2b-08a0-4aa5-b24c-6b657326a965"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=3cc26759b21a222ac22023974292d2bb8893c040d61090081156fbf6431340fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=6fe29cc09730e5a9628e031cc6b9ed19231a0970c31e78ecc87138c653471f49\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: sentry-sdk, smmap, gitdb, GitPython, pathtools, docker-pycreds, subprocess32, configparser, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxRkLboPxnhH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqd_v_yiiJMD",
        "outputId": "8612ab4d-33ad-4848-913e-2daa16a7b10c"
      },
      "source": [
        "import wandb\r\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnpKwPOyiOiK",
        "outputId": "3777e8dd-4486-413e-bd1e-14a53b765a70"
      },
      "source": [
        "#importing the fasihion mnist dataset from keras\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxqKNYSNidYq"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.colors\r\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_0Br5Pnjdp3"
      },
      "source": [
        "def sigmoid(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        if(x[i]>=0):\r\n",
        "            temp[i]=1.0/(1.0+np.exp(-x[i]))\r\n",
        "        else:\r\n",
        "            temp[i]=np.exp(x[i])/(1.0+np.exp(x[i]))\r\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqUUIJOXkHc-"
      },
      "source": [
        "def sigmoid_dif(x):\r\n",
        "    return x*(1-x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl1Gx7PVkJ3N"
      },
      "source": [
        "def tanh(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        temp[i]=np.exp(x[i])-np.exp(-x[i])/np.exp(x[i])+np.exp(-x[i])\r\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tEZpDMbkWR6"
      },
      "source": [
        "def tanh_dif(x):\r\n",
        "    return (1 - (x)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGMLh35LkY7I"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def relu(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        if x[i]>0:\r\n",
        "            temp[i]=x[i]\r\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ON1PRhNkaY_"
      },
      "source": [
        "\r\n",
        "def relu_dif(x):\r\n",
        "    temp=np.zeros((len(x),1))\r\n",
        "    for i in range(len(x)):\r\n",
        "        if x[i]>0:\r\n",
        "            temp[i]=1\r\n",
        "    return temp\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4NVDZf0kcA4"
      },
      "source": [
        "def softmax(x):\r\n",
        "    mx=np.max(x)\r\n",
        "    x=x-mx\r\n",
        "    return np.exp(x)/np.sum(np.exp(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa2zQAxJk2YA"
      },
      "source": [
        "def initialize_network(layer_size_list,mthd):\r\n",
        "    weights={}\r\n",
        "    bias={}\r\n",
        "    if mthd=='random':\r\n",
        "        for i,x,y in zip(range(len(layer_size_list)-1),layer_size_list[:-1],layer_size_list[1:]):\r\n",
        "            weights[i+1]=np.random.randn(y,x)\r\n",
        "        for i,x in zip(range(len(layer_size_list)-1),layer_size_list[1:]):\r\n",
        "            bias[i+1]=np.zeros((x,1))\r\n",
        "        return weights,bias\r\n",
        "    elif mthd=='xavier':\r\n",
        "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
        "        for i,x,y in zip(range(len(layer_size_list)-1),layer_size_list[:-1],layer_size_list[1:]):\r\n",
        "            weights[i+1]=np.array(initializer(shape=(y, x)))\r\n",
        "        for i,x in zip(range(len(layer_size_list)-1),layer_size_list[1:]):\r\n",
        "            bias[i+1]=np.zeros((x,1))\r\n",
        "        return weights,bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83yCWbEblD9f"
      },
      "source": [
        "def forward_propagation(x,weights,bias,mthd):\r\n",
        "    a={}\r\n",
        "    h={}\r\n",
        "    h[0]=x\r\n",
        "    layers=len(weights)\r\n",
        "    if mthd=='sigmoid':\r\n",
        "        for i in range(layers-1):\r\n",
        "            a[i+1]=np.matmul(weights[i+1],h[i])+bias[i+1]\r\n",
        "            h[i+1]=sigmoid(a[i+1])\r\n",
        "        a[layers]=np.matmul(weights[layers],h[layers-1])+bias[layers]\r\n",
        "        h[layers]=softmax(a[layers])\r\n",
        "        #return h[layers]\r\n",
        "        return a,h\r\n",
        "    elif mthd=='tanh':\r\n",
        "        for i in range(layers-1):\r\n",
        "            a[i+1]=np.matmul(weights[i+1],h[i])+bias[i+1]\r\n",
        "            h[i+1]=tanh(a[i+1])\r\n",
        "        a[layers]=np.matmul(weights[layers],h[layers-1])+bias[layers]\r\n",
        "        h[layers]=softmax(a[layers])\r\n",
        "        #return h[layers]\r\n",
        "        return a,h\r\n",
        "    elif mthd=='relu':\r\n",
        "        for i in range(layers-1):\r\n",
        "            a[i+1]=np.matmul(weights[i+1],h[i])+bias[i+1]\r\n",
        "            h[i+1]=relu(a[i+1])\r\n",
        "        a[layers]=np.matmul(weights[layers],h[layers-1])+bias[layers]\r\n",
        "        h[layers]=softmax(a[layers])\r\n",
        "        #return h[layers]\r\n",
        "        return a,h\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RIlKNf7lfAi"
      },
      "source": [
        "def back_propagation(x,y,weights,bias,mthd):\r\n",
        "    a,h=forward_propagation(x.reshape(784,1),weights,bias,mthd)\r\n",
        "    dW={}\r\n",
        "    dB={}\r\n",
        "    dH={}\r\n",
        "    dA={}\r\n",
        "    layers=len(weights)\r\n",
        "    y_label=[0]*10\r\n",
        "    y_label[y]=1\r\n",
        "    y_label=np.array(y_label).reshape(10,1)\r\n",
        "    dA[layers]=h[layers].reshape(10,1)-y_label\r\n",
        "    for i in range(layers,0,-1):\r\n",
        "        dW[i]=np.matmul(dA[i],h[i-1].T)\r\n",
        "        dB[i]=dA[i]\r\n",
        "        dH[i-1]=np.matmul(weights[i].T,dA[i])\r\n",
        "        if mthd=='sigmoid':\r\n",
        "            dA[i-1]=np.multiply(dH[i-1],sigmoid_dif(h[i-1]))\r\n",
        "        elif mthd=='tanh':\r\n",
        "            dA[i-1]=np.multiply(dH[i-1],tanh_dif(h[i-1]))\r\n",
        "        elif mthd=='relu':\r\n",
        "            dA[i-1]=np.multiply(dH[i-1],relu_dif(h[i-1]))\r\n",
        "    return dW,dB    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkBJfy-mEVN"
      },
      "source": [
        "#Optimization Functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a4QNcuMlrBW"
      },
      "source": [
        "def stochastic_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0,batch_size=10):\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        num_point_seen=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:1000],train_labels[0:1000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "            num_point_seen+=1\r\n",
        "            if num_point_seen==batch_size:\r\n",
        "                for z in range(len(layers_size_list)-1):\r\n",
        "                    weights[z+1]=weights[z+1]-learning_rate*dw[z+1]/batch_size-learning_rate*alpha*weights[z+1]\r\n",
        "                    bias[z+1]=bias[z+1]-learning_rate*db[z+1]/batch_size\r\n",
        "                for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "                    dw[i+1]=np.zeros((y,x))\r\n",
        "                for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "                    db[i+1]=np.zeros((x,1))\r\n",
        "                num_point_seen=0\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:1000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:1000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        for i in range(len(train_labels[0:1000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:1000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:1000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train/len(train_labels[0:1000]),\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val/len(test_labels),\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzSS97SkmMGu"
      },
      "source": [
        "\r\n",
        "def momentum_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        prev_vw={}\r\n",
        "        prev_vb={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "            prev_vw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "            prev_vb[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:1000],train_labels[0:1000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        m=len(train_images[0:1000])\r\n",
        "        v_w={}\r\n",
        "        v_b={}\r\n",
        "        gamma=0.9\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            v_w[i+1]=gamma*prev_vw[i+1]+learning_rate*dw[i+1]#/m\r\n",
        "            v_b[i+1]=gamma*prev_vb[i+1]+learning_rate*db[i+1]#/m\r\n",
        "            weights[i+1]=weights[i+1]-v_w[i+1]-learning_rate*alpha*weights[i+1] #this is l2 regularisation\r\n",
        "            bias[i+1]=bias[i+1]-v_b[i+1]\r\n",
        "            prev_vw[i+1]=v_w[i+1]\r\n",
        "            prev_vb[i+1]=v_b[i+1]\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:1000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:1000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        for i in range(len(train_labels[0:1000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:1000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:1000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train/len(train_labels[0:1000]),\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val/len(test_labels),\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWYuhwRxmk6t"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def nestrov_gradient_descent(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        prev_vw={}\r\n",
        "        prev_vb={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "            prev_vw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "            prev_vb[i+1]=np.zeros((x,1))\r\n",
        "        v_w={}\r\n",
        "        v_b={}\r\n",
        "        gamma=0.9\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            v_w[i+1]=gamma*prev_vw[i+1]\r\n",
        "            v_b[i+1]=gamma*prev_vb[i+1]\r\n",
        "        tempw={}\r\n",
        "        tempb={}\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            tempw[i+1]=weights[i+1]-v_w[i+1]\r\n",
        "            tempb[i+1]=bias[i+1]-v_b[i+1]\r\n",
        "        for j,k in zip(train_images[0:1000],train_labels[0:1000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,tempw,tempb,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        m=len(train_images[0:1000])\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            v_w[i+1]=gamma*prev_vw[i+1]+learning_rate*dw[i+1]#/m\r\n",
        "            v_b[i+1]=gamma*prev_vb[i+1]+learning_rate*db[i+1]#/m\r\n",
        "            weights[i+1]=weights[i+1]-v_w[i+1]-learning_rate*alpha*weights[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-v_b[i+1]\r\n",
        "            prev_vw[i+1]=v_w[i+1]\r\n",
        "            prev_vb[i+1]=v_b[i+1]\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:1000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:1000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        for i in range(len(train_labels[0:1000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:1000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:1000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train/len(train_labels[0:1000]),\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val/len(test_labels),\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZtKK_ism7YD"
      },
      "source": [
        "def rmsprop(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    res=[]\r\n",
        "    vw={}\r\n",
        "    vb={}\r\n",
        "    for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "        vw[i+1]=np.zeros((y,x))\r\n",
        "    for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "        vb[i+1]=np.zeros((x,1))\r\n",
        "    eps=1e-8\r\n",
        "    beta1=0.9\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:1000],train_labels[0:1000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        #m=len(train_images[0:1000])\r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw[l+1]=vw[l+1]*beta1+(1-beta1)*(dw[l+1]**2)\r\n",
        "            vb[l+1]=vb[l+1]*beta1+(1-beta1)*(db[l+1]**2)\r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            weights[i+1]=weights[i+1]-(learning_rate)*(dw[i+1]/np.sqrt(vw[i+1]+eps))-learning_rate*alpha*weights[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-(learning_rate)*(db[i+1]/np.sqrt(vb[i+1]+eps))\r\n",
        "        \r\n",
        "        #below code is for testing accuracy on train data\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:1000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:1000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        for i in range(len(train_labels[0:1000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:1000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:1000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train/len(train_labels[0:1000]),\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val/len(test_labels),\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57p1Jb0am-j_"
      },
      "source": [
        "def adam(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    res=[]\r\n",
        "    vw={}\r\n",
        "    vb={}\r\n",
        "    mw={}\r\n",
        "    mb={}\r\n",
        "    mw_hat={}\r\n",
        "    mb_hat={}\r\n",
        "    vw_hat={}\r\n",
        "    vb_hat={}\r\n",
        "    for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "        vw[i+1]=np.zeros((y,x))\r\n",
        "        mw[i+1]=np.zeros((y,x))\r\n",
        "        mw_hat[i+1]=np.zeros((y,x))\r\n",
        "        vw_hat[i+1]=np.zeros((y,x))\r\n",
        "        \r\n",
        "    for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "        vb[i+1]=np.zeros((x,1))\r\n",
        "        mb[i+1]=np.zeros((x,1))\r\n",
        "        mb_hat[i+1]=np.zeros((x,1))\r\n",
        "        vb_hat[i+1]=np.zeros((x,1))\r\n",
        "\r\n",
        "    eps=1e-8\r\n",
        "    beta1=0.9\r\n",
        "    beta2=0.999\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:1000],train_labels[0:1000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        #m=len(train_images[0:1000])\r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw[l+1]=mw[l+1]*beta1+(1-beta1)*(dw[l+1])\r\n",
        "            mb[l+1]=mb[l+1]*beta1+(1-beta1)*(db[l+1])\r\n",
        "            \r\n",
        "        \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw[l+1]=vw[l+1]*beta2+(1-beta2)*(dw[l+1]**2)\r\n",
        "            vb[l+1]=vb[l+1]*beta2+(1-beta2)*(db[l+1]**2)\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw_hat[l+1]=mw[l+1]/(1-(beta1**(e+1)))\r\n",
        "            mb_hat[l+1]=mb[l+1]/(1-(beta1**(e+1)))\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw_hat[l+1]=vw[l+1]/(1-(beta2**(e+1)))\r\n",
        "            vb_hat[l+1]=vb[l+1]/(1-(beta2**(e+1)))\r\n",
        "            \r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            weights[i+1]=weights[i+1]-(learning_rate)*(mw_hat[i+1]/np.sqrt(vw_hat[i+1]+eps))-learning_rate*alpha*weights[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-(learning_rate)*(mb_hat[i+1]/np.sqrt(vb_hat[i+1]+eps))\r\n",
        "        \r\n",
        "        #below code is for testing accuracy on train data\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:1000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:1000])\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        for i in range(len(train_labels[0:1000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:1000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:1000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train/len(train_labels[0:1000]),\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val/len(test_labels),\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFDKpJ0cnBtY"
      },
      "source": [
        "def nadam(weights,bias,epochs,layers_size_list,train_images,train_labels,learning_rate,mthd,alpha=0):\r\n",
        "    res=[]\r\n",
        "    vw={}\r\n",
        "    vb={}\r\n",
        "    mw={}\r\n",
        "    mb={}\r\n",
        "    mw_hat={}\r\n",
        "    mb_hat={}\r\n",
        "    vw_hat={}\r\n",
        "    vb_hat={}\r\n",
        "    for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "        vw[i+1]=np.zeros((y,x))\r\n",
        "        mw[i+1]=np.zeros((y,x))\r\n",
        "        mw_hat[i+1]=np.zeros((y,x))\r\n",
        "        vw_hat[i+1]=np.zeros((y,x))\r\n",
        "        \r\n",
        "    for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "        vb[i+1]=np.zeros((x,1))\r\n",
        "        mb[i+1]=np.zeros((x,1))\r\n",
        "        mb_hat[i+1]=np.zeros((x,1))\r\n",
        "        vb_hat[i+1]=np.zeros((x,1))\r\n",
        "\r\n",
        "    eps=1e-8\r\n",
        "    beta1=0.9\r\n",
        "    beta2=0.999\r\n",
        "    for e in range(epochs):\r\n",
        "        dw={}\r\n",
        "        db={}\r\n",
        "        ce_train=0\r\n",
        "        ce_val=0\r\n",
        "        for i,x,y in zip(range(len(layers_size_list)-1),layers_size_list[:-1],layers_size_list[1:]):\r\n",
        "            dw[i+1]=np.zeros((y,x))\r\n",
        "        for i,x in zip(range(len(layers_size_list)-1),layers_size_list[1:]):\r\n",
        "            db[i+1]=np.zeros((x,1))\r\n",
        "        for j,k in zip(train_images[0:1000],train_labels[0:1000]):\r\n",
        "            tdw,tdb=back_propagation(j,k,weights,bias,mthd)\r\n",
        "            for l in range(len(layers_size_list)-1):\r\n",
        "                dw[l+1]+=tdw[l+1]\r\n",
        "                db[l+1]+=tdb[l+1]\r\n",
        "        #m=len(train_images[0:1000])\r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw[l+1]=mw[l+1]*beta1+(1-beta1)*(dw[l+1])\r\n",
        "            mb[l+1]=mb[l+1]*beta1+(1-beta1)*(db[l+1])\r\n",
        "            \r\n",
        "        \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw[l+1]=vw[l+1]*beta2+(1-beta2)*(dw[l+1]**2)\r\n",
        "            vb[l+1]=vb[l+1]*beta2+(1-beta2)*(db[l+1]**2)\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            mw_hat[l+1]=mw[l+1]/(1-(beta1**(e+1)))\r\n",
        "            mb_hat[l+1]=mb[l+1]/(1-(beta1**(e+1)))\r\n",
        "            \r\n",
        "        for l in range(len(layers_size_list)-1):\r\n",
        "            vw_hat[l+1]=vw[l+1]/(1-(beta2**(e+1)))\r\n",
        "            vb_hat[l+1]=vb[l+1]/(1-(beta2**(e+1)))\r\n",
        "        #xk = xk - (n/(vc_k**0.5 + epsilon))*(b1*mc_k + (1-b1)*gt/(1-b1**iter_count))  \r\n",
        "        for i in range(len(layers_size_list)-1):\r\n",
        "            numrw=beta1*mw_hat[i+1]+(1-beta1)/(1-beta1**(e+1))*dw[i+1]\r\n",
        "            weights[i+1]=weights[i+1]-(learning_rate)*(numrw/np.sqrt(vw_hat[i+1]+eps))-learning_rate*alpha*weights[i+1]\r\n",
        "            \r\n",
        "            numrb=beta1*mb_hat[i+1]+(1-beta1)/(1-beta1**(e+1))*db[i+1]\r\n",
        "            bias[i+1]=bias[i+1]-(learning_rate)*(numrb/np.sqrt(vb_hat[i+1]+eps))\r\n",
        "        \r\n",
        "        #below code is for testing accuracy on train data\r\n",
        "        res=[]\r\n",
        "        for i in train_images[0:1000]:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res.append(h[len(weights)])\r\n",
        "        y_pred=[]\r\n",
        "        for i in res:\r\n",
        "            j=list(i)\r\n",
        "            temp=j.index(max(j))\r\n",
        "            y_pred.append(temp)\r\n",
        "        train_acc=accuracy_score(y_pred, train_labels[0:1000])\r\n",
        "        print(e+1,\":\",train_acc)\r\n",
        "        #the below code is for calculating cross entropy\r\n",
        "        for i in range(len(train_labels[0:1000])):\r\n",
        "            j=[0]*10\r\n",
        "            j[train_labels[i]]=1\r\n",
        "            ce_train+=cross_entropy(res[i],j)\r\n",
        "        res_test=[]\r\n",
        "        for i in test_images:\r\n",
        "            a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "            res_test.append(h[len(weights)])\r\n",
        "        for i in range(len(test_labels)):\r\n",
        "            j=[0]*10\r\n",
        "            j[test_labels[i]]=1\r\n",
        "            ce_val+=cross_entropy(res_test[i],j)\r\n",
        "            #mse1+=mse(res[i],j)\r\n",
        "        #print(\"CE-loss:\",ce/len(train_labels[0:1000]))\r\n",
        "        #print(\"MSE-loss:\",mse1/len(train_labels[0:1000]))\r\n",
        "        #if((e+1)%100==0):\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "        test_acc=test_accuracy(weights,bias,test_images,test_labels,mthd)\r\n",
        "        wandb.log({\r\n",
        "        \"Epoch\": e,\r\n",
        "        \"Train Loss\": ce_train/len(train_labels[0:1000]),\r\n",
        "        \"Train Acc\": train_acc,\r\n",
        "        \"Valid Loss\": ce_val/len(test_labels),\r\n",
        "        \"Valid Acc\": test_acc})\r\n",
        "        #res.append(train_acc)\r\n",
        "        #print(e,\":\",train_acc)\r\n",
        "    return weights,bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-vhrVgCntHH"
      },
      "source": [
        "def mse(pred,label):\r\n",
        "    label=np.array(label).reshape(10,1)\r\n",
        "    mse=0\r\n",
        "    for i in range(len(label)):\r\n",
        "        mse+=(pred[i]-label[i])**2\r\n",
        "    return mse[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvfpvsQJoPeH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def cross_entropy(pred,label):\r\n",
        "    yl=np.multiply(pred,np.array(label).reshape(10,1))\r\n",
        "    yl=yl[yl!=0]\r\n",
        "    yl=-np.log(yl)\r\n",
        "    #yl=np.mean(yl)\r\n",
        "    return yl[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-SGBv5LoWQl"
      },
      "source": [
        "def test_accuracy(weights,bias,test_images,test_labels,mthd):\r\n",
        "    res=[]\r\n",
        "    for i in test_images:\r\n",
        "        a,h=forward_propagation(i.reshape(784,1),weights,bias,mthd)\r\n",
        "        res.append(h[len(weights)])\r\n",
        "    y_pred=[]\r\n",
        "    for i in res:\r\n",
        "        j=list(i)\r\n",
        "        temp=j.index(max(j))\r\n",
        "        y_pred.append(temp)\r\n",
        "    test_acc=accuracy_score(y_pred, test_labels)\r\n",
        "    return test_acc"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}